<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sevncz.xyz","root":"/","images":"/images","scheme":"Gemini","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="努力赚钱">
<meta property="og:type" content="website">
<meta property="og:title" content="Sevncz&#39;s Blog">
<meta property="og:url" content="http://sevncz.xyz/index.html">
<meta property="og:site_name" content="Sevncz&#39;s Blog">
<meta property="og:description" content="努力赚钱">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sevncz">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://sevncz.xyz/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>
<title>Sevncz's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Sevncz's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-bigdata">

    <a href="/categories/bigdata/" rel="section"><i class="fa fa-project-diagram fa-fw"></i>大数据</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sevncz</p>
  <div class="site-description" itemprop="description">努力赚钱</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">6</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Sevncz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sevncz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/HDFS%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/HDFS%E5%8E%9F%E7%90%86%E6%B5%85%E6%9E%90/" class="post-title-link" itemprop="url">HDFS原理浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:54:43" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="HDFS原理浅析"><a href="#HDFS原理浅析" class="headerlink" title="HDFS原理浅析"></a>HDFS原理浅析</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul>
<li>HDFS集群分为两大角色：NameNode、DataNode</li>
<li>NameNode负责管理整个文件系统和每一个文件的元数据（元数据是block的ID，存储在哪一个DataNode）</li>
<li>DataNode负责管理用户的文件数据块</li>
<li>文件会按照固定的大小（BlockSize）切成若干块后分布式存储在若干台DataNode上；默认大小：bock块在2.x版本中是128M，老版本中是64M</li>
<li>每一个文件块可以有多个副本，并存放在不同的DataNode上</li>
<li>DataNode会定期向NameNode汇报自身所保存的文件Block信息，而NameNode则会负责保持文件的副本数量</li>
<li>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过NameNode申请来进行</li>
</ul>
<h2 id="HDFS写文件步骤"><a href="#HDFS写文件步骤" class="headerlink" title="HDFS写文件步骤"></a>HDFS写文件步骤</h2><p>客户端要向HDFS写数据，首先要跟NameNode通信以确认可以写文件并获得接收文件的Block和DataNode。然后，客户端按顺序将文件逐个Block传递给相应的DataNode，并由接收到Block的DataNode负责向其他DataNode复制Block的副本。</p>
<p><strong>详细步骤：</strong></p>
<ol>
<li>Client向NameNode通信请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在；</li>
<li>NameNode返回是否可以上传；</li>
<li>Client请求第一个Block该传输到哪些DataNode服务器上；</li>
<li>NameNode返回3个DataNode服务器；</li>
<li>Client请求3台DataNode中的一台上传数据（本质是一个RPC调用，建立Pipeline），服务器接收到一个Packet就会传给第二台服务器，第二台服务器传给第三台服务器；服务器每传一个Packet会放入一个应答队列等待应答；</li>
<li>当一个Block传输完成之后，Client再次请求Namenode上传第二个Block的服务器。</li>
</ol>
<h2 id="HDFS读文件步骤"><a href="#HDFS读文件步骤" class="headerlink" title="HDFS读文件步骤"></a>HDFS读文件步骤</h2><p>客户端将要读取的文件路径发送给NameNode，NameNode获取文件的元信息返回给客户端，客户端根据返回的信息找到相应的DataNode逐个获取文件的Block并在客户端本地进行数据追加合并从而获得整个文件。</p>
<p><strong>详细步骤：</strong></p>
<ol>
<li>Client向NameNode通信请求读取文件；NameNode查询元数据，找到文件块所在的DataNode服务器；</li>
<li>挑选一台DataNode（就近原则，然后随机）服务器，请求建立Socket连接；</li>
<li>DataNode开发发送数据（从磁盘里面读取数据放入流，以Packet为单位来做校验）；</li>
<li>Client以Packet为单位接收，现在本地缓存，然后写入目标文件</li>
</ol>
<h2 id="NameNode工作机制"><a href="#NameNode工作机制" class="headerlink" title="NameNode工作机制"></a>NameNode工作机制</h2><h3 id="NameNode工作职责"><a href="#NameNode工作职责" class="headerlink" title="NameNode工作职责"></a>NameNode工作职责</h3><ul>
<li><p>负责客户端请求的相应；</p>
</li>
<li><p>元数据的管理（查询，修改）；</p>
<p>namenode对数据的管理采用了三种存储形式：</p>
<ul>
<li>内存元数据(NameSystem)</li>
<li>磁盘元数据镜像文件</li>
<li>数据操作日志文件（可通过日志运算出元数据）</li>
</ul>
</li>
</ul>
<h3 id="元数据存储机制"><a href="#元数据存储机制" class="headerlink" title="元数据存储机制"></a>元数据存储机制</h3><p>A. 内存中有一份完整的元数据(内存meta data)</p>
<p>B. 磁盘有一个“准完整”的元数据镜像（fsimage）文件(在NameNode的工作目录中)</p>
<p>C. 用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）</p>
<blockquote>
<p>注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中</p>
</blockquote>
<h3 id="元数据手动查看"><a href="#元数据手动查看" class="headerlink" title="元数据手动查看"></a>元数据手动查看</h3><p>可以通过hdfs的一个工具来查看edits中的信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin&#x2F;hdfs oev -i edits -o edits.xml&#96; </span><br><span class="line">bin&#x2F;hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</span><br></pre></td></tr></table></figure>

<h3 id="元数据的checkpoin"><a href="#元数据的checkpoin" class="headerlink" title="元数据的checkpoin"></a>元数据的checkpoin</h3><p>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</p>
<p><strong>checkpoint的详细过程：</strong></p>
<p><strong>checkpoint触发条件配置参数：</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">dfs.namenode.checkpoint.checkpoint.period</span>=<span class="string">60 #检查触发条件是否满足的频率，60秒</span></span><br><span class="line"><span class="meta">dfs.namenode.checkpoint.checkpoint.dir</span>=<span class="string">file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary</span></span><br><span class="line"><span class="meta">dfs.namenode.checkpoint.edits.dir</span>=<span class="string">$&#123;dfs.namenode.checkpoint.dir&#125; #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录</span></span><br><span class="line"><span class="meta">dfs.namenode.checkpoint.max-retries</span>=<span class="string">3 #最大重试次数</span></span><br><span class="line"><span class="meta">dfs.namenode.checkpoint.period</span>=<span class="string">3600 #两次checkpoint之间的时间间隔3600秒</span></span><br><span class="line"><span class="meta">dfs.namenode.checkpoint.txns</span>=<span class="string">1000000 #两次checkpoint之间最大的操作记录</span></span><br></pre></td></tr></table></figure>

<p><strong>checkpoint的附带作用</strong></p>
<p>namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p>
<h2 id="DataNode的工作机制"><a href="#DataNode的工作机制" class="headerlink" title="DataNode的工作机制"></a>DataNode的工作机制</h2><h3 id="DataNode工作职责："><a href="#DataNode工作职责：" class="headerlink" title="DataNode工作职责："></a>DataNode工作职责：</h3><ul>
<li>存储管理用户的文件块数据</li>
<li>定期向NameNode汇报自身所持有的Block信息（通过心跳信息上报）（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）</li>
</ul>
<h3 id="Datanode掉线判断时限参数"><a href="#Datanode掉线判断时限参数" class="headerlink" title="Datanode掉线判断时限参数"></a>Datanode掉线判断时限参数</h3><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为:<br><code>timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval</code></p>
<p>而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。<br>需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。</p>
<h2 id="fsimage-和-edits"><a href="#fsimage-和-edits" class="headerlink" title="fsimage 和 edits"></a>fsimage 和 edits</h2><h4 id="fsimage"><a href="#fsimage" class="headerlink" title="fsimage"></a>fsimage</h4><p>NameNode 的元数据检查点，里面记录了自最后一次检查点之前HDFS文件系统中所有目录和文件的序列化信息</p>
<h4 id="edits"><a href="#edits" class="headerlink" title="edits"></a>edits</h4><p>保存了自最后一次检查点之后所有针对HDFS文件系统的操作，比如：增加文件、重命名文件、删除目录等等</p>
<h4 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h4><ol>
<li>配置好HA后，客户端所有的更新操作将会写到JournalNode节点的共享目录中</li>
<li>Active NameNode和Standby NameNode从JournalNpde的edits共享目录同步到自己edits目录中</li>
<li>Standby NameNode中的StandbyCheckpointer类会定期的检查合并的条件是否成立，如果成立会合并fsimage和edits文件</li>
<li>Standby NameNode中的StandbyCheckpointer类合并完之后，将合并之后的fsimage上传到Active NameNode相应目录中</li>
<li>Active NameNode接到最新的fsimage文件之后，将旧的fsimage和edits文件清理掉</li>
<li>通过上面的几步，fsimage和edits文件就完成了合并，由于HA机制，会使得Standby NameNode和Active NameNode都拥有最新的fsimage和edits文件（之前Hadoop 1.x的SecondaryNameNode中的fsimage和edits不是最新的）</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/Hive/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/Hive/" class="post-title-link" itemprop="url">Hive原理浅析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:54:59" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/MapReduce%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/MapReduce%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">MapReduce原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:55:10" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MapReduce原理"><a href="#MapReduce原理" class="headerlink" title="MapReduce原理"></a>MapReduce原理</h1><p>MapReduce 即是编程模型也是计算框架。开发人员必须基于MapReduce 编程模型进行开发，然后将程序通过MapReduce计算框架分发到Hadoop集群中运行。</p>
<p>大数据计算的核心思想是移动计算，它比移动数据更划算，传统的编程模型进行大数据计算就会遇到很多困难，因此Hadoop大数据计算使用这种MapReduce的编程模型。</p>
<h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>作为一种编程模型，该模型主要分为两个过程，即 <code>Map</code> 和 <code>Reduce</code>。 <code>MapReduce</code> 的整体思想是： <strong>将输入的数据分成 M 个 <code>tasks</code>， 由用户自定义的 <code>Map</code> 函数去执行任务，产出 <code>&lt;Key, Value&gt;</code>形式的中间数据，然后相同的 key 通过用户自定义的 <code>Reduce</code> 函数去聚合，得到最终的结果。</strong></p>
<p>MapReduce执行过程主要包括:</p>
<h3 id="模型实现"><a href="#模型实现" class="headerlink" title="模型实现"></a>模型实现</h3><p>具体过程如下：</p>
<p><strong>Map端</strong></p>
<ul>
<li>根据输入输入信息，将输入数据 split 成 M 份</li>
<li>在所有可用的<code>worker</code>节点中，起 M 个<code>task</code>任务的线程， 每个任务会读取对应一个 split 当做输入。</li>
<li>调用 <code>map</code> 函数，将输入转化为 <code>&lt;Key, Value&gt;</code> 格式的中间数据，并且排序后，写入磁盘。 这里，每个 <code>task</code> 会写 R 个文件，对应着 <code>Reduce</code> 任务数。 数据写入哪个文件的规则有 <code>Partitioner</code> 决定，默认是 <code>hash(key) % R</code></li>
<li>(可选) 为了优化性能，中间还可以用一个 <code>combiner</code> 的中间过程</li>
</ul>
<p><strong>Reduce 端</strong></p>
<ul>
<li><p><code>map</code> 阶段结束以后， 开始进入 <code>Reduce</code> 阶段，每个 <code>Reduce task</code>会从所有的 <code>Map</code> 中间数据中，获取属于自己的一份数据，拿到所有数据后，一般会进行排序<code>(Hadoop 框架是这样做)</code> 。</p>
<blockquote>
<p>说明： 这个排序是非常必要的，主要因为 <code>Reduce</code> 函数的输入 是 <code>&lt;key, []values&gt;</code> 的格式，因为需要根据key去排序。为啥不用 <code>map&lt;&gt;()</code> 去实现呢？ 原因：因为map必须存到内存中，但是实际中数据量很大，往往需要溢写到磁盘。 但是排序是可以做到的，比如归并排序。 这也就是map端产出数据需要排序，Reduce端获取数据后也需要先排序的原因。</p>
</blockquote>
</li>
<li><p>调用 <code>Reduce</code> 函数，得到最终的结果输出结果，存入对应的文件</p>
</li>
<li><p>(可选) 汇总所有 <code>Reduce</code>任务的结果。一般不做汇总，因为通常一个任务的结果往往是另一个 <code>MapReduce</code>任务的输入，因此没必要汇总到一个文件中。</p>
</li>
</ul>
<h3 id="Master-数据结构"><a href="#Master-数据结构" class="headerlink" title="Master 数据结构"></a>Master 数据结构</h3><p><code>master</code> 是 <code>MapReduce</code> 任务中最核心的角色，它需要维护 <strong>状态信息</strong> 和 <strong>文件信息</strong>。</p>
<ul>
<li>状态信息：<ul>
<li><code>map</code> 任务状态</li>
<li><code>Reduce</code> 任务状态</li>
<li><code>worker</code> 节点状态</li>
</ul>
</li>
<li>文件信息<ul>
<li>输入文件信息</li>
<li>输出文件信息</li>
<li><code>map</code>中间数据文件信息</li>
</ul>
</li>
</ul>
<blockquote>
<p>由于 <code>master</code> 节点进程是整个任务的枢纽，因此，它需要维护输入文件地址，<code>map</code>任务执行完后，会产出中间数据文件等待 <code>reducer</code> 去获取，因此 <code>map</code>完成后, 会向 <code>master</code> 上报这些文件的位置和大小信息，这些信息随着<code>Reduce</code>任务的启动而分发下推到对应的 <code>worker</code>。</p>
</blockquote>
<h3 id="容错"><a href="#容错" class="headerlink" title="容错"></a>容错</h3><h4 id="worker-节点失败"><a href="#worker-节点失败" class="headerlink" title="worker 节点失败"></a>worker 节点失败</h4><p>master 会周期性向所有节点发送 ping 心跳检测，如果超时未回复，master 会认为该 worker 已经故障。任何在该节点完成的 map 或者 reduce 任务都会标记为 idle，并由其他的 worker 重新执行。</p>
<blockquote>
<p>说明： 因为<code>MapReduce</code> 为了减少网络带宽的消耗，<code>map</code>的数据是存储在本地磁盘的，如果某个<code>worker</code>机器故障，会导致其他的<code>Reduce</code> 任务拿不到对应的中间数据，所以需要重跑任务。那么这也可以看出，如果利用<code>hadoop</code> 等分布式文件系统来存储中间数据，其实对于完成的<code>map</code>任务，是不需要重跑的，代价就是增加网络带宽。</p>
</blockquote>
<h4 id="Master-节点失败"><a href="#Master-节点失败" class="headerlink" title="Master 节点失败"></a>Master 节点失败</h4><p>master 节点失败，在没有实现HA的情况下，可以说基本整个MapReduce任务就已经挂了，对于这种情况，直接重启 master 重跑任务就OK了。如果集群有高可用方案，比如 master 主副备用，就可以实现 master 的高可用，<strong>代价就是得同步维护主副之间的状态信息和文件信息等。</strong></p>
<h4 id="失败处理的语义"><a href="#失败处理的语义" class="headerlink" title="失败处理的语义"></a>失败处理的语义</h4><p>只要<code>Map</code> <code>Reduce</code>函数是确定的，语义上不管是分布式执行还是单机执行，结果都是一致的。每个<code>map</code> <code>Reduce</code> 任务输出是通过原子提交来保证的， 即：<strong>一个任务要么有完整的最终文件，要么存在最终输出结果，要么不存在。</strong></p>
<ul>
<li>每个进行中的任务，在没有最终语义完成之前，都只写临时文件，每个<code>Reduce</code> 任务会写一个，而每个<code>Map</code>任务会写 R 个，对应 R 个<code>reducer</code>.</li>
<li>当 <code>Map</code> 任务完成的时候，会向<code>master</code>发送文件位置，大小等信息。<code>Master</code>如果接受到一个已经完成的<code>Map</code>任务的信息，就忽略掉，否则，会记录这个信息。</li>
<li>当 <code>Reduce</code> 任务完成的时候，会将临时文件重命名为最终的输出文件， 如果多个相同的<code>Reduce</code>任务在多台机器执行完，会多次覆盖输出文件，这个由底层文件系统的<code>rename</code>操作的原子性，保证任何时刻，看到的都是一个完整的成功结果</li>
</ul>
<p>对于大部分确定性的任务，不管是分布式还是串行执行，最终都会得到一致的结果。对于不确定的<code>map</code> 或者<code>Reduce</code> 任务，<code>MapReduce</code> 保证提供一个弱的，仍然合理的语义。</p>
<h4 id="存储优化"><a href="#存储优化" class="headerlink" title="存储优化"></a>存储优化</h4><p>由于网络带宽资源的昂贵性，因此对<code>MapReduce</code> 存储做了很多必要的优化。</p>
<ul>
<li>通过从本地磁盘读取文件，节约网络带宽</li>
<li>GFS 将文件分解成多个 大小通常为 64M 的<code>block</code>, 并多备份存储在不同的机器上，在调度时，会考虑文件的位置信息，尽可能在存有输入文件的机器上调度<code>map</code>任务，避免网络IO。</li>
<li>任务失败时，也会尝试在离副本最近的worker中执行，比如同一子网下的机器。</li>
<li>MapReduce 任务在大集群中执行时，大部分输入直接可以从磁盘中读取，不消耗带宽。</li>
</ul>
<h4 id="任务粒度"><a href="#任务粒度" class="headerlink" title="任务粒度"></a>任务粒度</h4><p>通常情况下，任务数即为O(M+R)，这个数量应当比<code>worker</code>数量多得多，这样利于负载均衡和失败恢复的情况，但是也不能无限增长，因为太多任务的调度，会消耗<code>master</code> 存储任务信息的内存资源，如果启动task所花的时间比任务执行时间还多，那就不偿失了。</p>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><h4 id="自定义分区函数（partition）"><a href="#自定义分区函数（partition）" class="headerlink" title="自定义分区函数（partition）"></a>自定义分区函数（partition）</h4><p>自定义分区可以更好地符合业务和进行负载均衡，防止数据倾斜。 默认只是简单的 <code>hash(key) % R</code></p>
<h4 id="有序保证"><a href="#有序保证" class="headerlink" title="有序保证"></a>有序保证</h4><p>每个partition内的数据都是排序的，这样有利于Reduce阶段的merge合并</p>
<h4 id="Combiner-函数"><a href="#Combiner-函数" class="headerlink" title="Combiner 函数"></a>Combiner 函数</h4><p>这个是每个<code>map</code>阶段完成之后，局部先做一次聚合。比如：词频统计，每个 Word 可能出现了100次，如果不使用<code>combiner</code>， 就会发送100 个 <code>&lt;word, 1&gt;</code>, 如果<code>combiner</code>聚合之后，则为 <code>&lt;word, 100&gt;</code>, 大大地减少了网络传输和磁盘的IO。</p>
<h4 id="输入输出类型"><a href="#输入输出类型" class="headerlink" title="输入输出类型"></a>输入输出类型</h4><p>一个<code>reader</code>没必要非要从文件读数据，<code>MapReduce</code> 支持可以从不同的数据源中以多种不同的方式读取数据，比如从数据库读取，用户只需要自定义split规则，就能轻易实现。</p>
<h4 id="计数器"><a href="#计数器" class="headerlink" title="计数器"></a>计数器</h4><p><code>MapReduce</code> 还添加了计数器，可以用来检测<code>MapReduce</code>的一些中间操作。</p>
<h2 id="计算框架"><a href="#计算框架" class="headerlink" title="计算框架"></a>计算框架</h2><p>MapReduce 流程：input &gt; Splitting &gt; Mapping &gt; Shuffling &gt; Reducing &gt; Result</p>
<p>MapReduce程序读取的数据，都是存储在HDFS的数据，最后的结果，也是要保存在HDFS中，因此，MapReduce要解决的第一个问题就是数据的切分问题。</p>
<p>Split大小的计算</p>
<ul>
<li>max.split(100M)</li>
<li>min.split(10M)</li>
<li>block(64M)</li>
<li>max(min.split, min(max.split, block))</li>
</ul>
<h3 id="Shuffle"><a href="#Shuffle" class="headerlink" title="Shuffle"></a>Shuffle</h3><p>处于map和reduce之间，该阶段是最重要的阶段，也是最难的阶段。</p>
<h4 id="input"><a href="#input" class="headerlink" title="input"></a>input</h4><p>map 执行的时候，数据来源是HDFS上的block，在MapReduce概念中，读取的是split，split和block对应的。</p>
<h4 id="map阶段"><a href="#map阶段" class="headerlink" title="map阶段"></a>map阶段</h4><p>经过自定义的 map 函数的处理后，结果以 key/value 的形式输出，但是这些结果要送到以后的哪一个 reduce 去执行，需要 partition 来决定。</p>
<h4 id="partition"><a href="#partition" class="headerlink" title="partition"></a>partition</h4><p>根据 key 或者 value 的值，以及 reduce 的数量，来决定当前的这对输出数据要交到那个 reduce 去处理。</p>
<p>默认方法：对 key hash 以后再以 reduce 的数量取模。</p>
<p>也可以由程序自定义 partition 函数，partition 只是对数据以后要被送到哪个 reduce 去处理做了一个标注，而不是立马把数据进行分区。</p>
<p>数据倾斜和负载均衡：默认的 partition 是可能产生数据倾斜和负载均衡，如果产生数据倾斜，就需要重新定义 partition 的分区规则，就可以避免数据倾斜问题。</p>
<h5 id="内存缓冲区的概念"><a href="#内存缓冲区的概念" class="headerlink" title="内存缓冲区的概念"></a>内存缓冲区的概念</h5><p>==每个map任务都有一个内存缓冲区，用于存储任务的输出，默认缓冲区的大小是100M，这个值是可以通过io.sort.mb来调整。由于缓冲区的空间大小有限，所以，当map task的输出结果很多的时候，内存缓冲区就装不下这么多的数据，也就需要将数据写到磁盘去。因此需要一个阈值（io.sort.spill.percent，默认是80%），当内存达到阈值以后，就会有一个单独的后台线程，负责将内存中的数据写到磁盘，这个过程叫做<strong>溢写**</strong>，**由于是由单独的线程来负责溢写，所以，溢写过程不会影响map结果的输出，但是，如果此期间缓冲区被写满，map就会阻塞知道写磁盘过程完成。==</p>
<p>这里，就有两个过程，一个写内存的过程，另外一个是写磁盘的过程。  </p>
<h4 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h4><p>这里的 sort 是按字典顺序排列，而不是按数值大小进行排序</p>
<h4 id="combine"><a href="#combine" class="headerlink" title="combine"></a>combine</h4><p>将排好序的数据，按照键相同的合并在一起的规则，进行值的合并。</p>
<p>map 的输出结果在经过 partition 阶段的处理后，明确了要发给哪个 reduce 去做处理，当写入内存后，需要将所有的键值对按照 key 进行 sort，在 sort 的基础上，再对结果进行 combine，最后，再写到磁盘文件上。所以，在磁盘上的数据，是已经分好区的，并且已经排好序的。</p>
<p><strong>溢写磁盘需要注意的地方：</strong>==如果map的输出结果很大，有多次溢写发生的话，磁盘上就会存在多个溢写文件（每次溢写都会产生一个溢写文件），在map task真正的完成是时，会将所有的溢写文件都Merge到一个溢写文件中，这个过程就叫Merge。比如，从一个map读取过来的是&lt;aaa, 5&gt;，另外一个map读取的 是&lt;aaa，8&gt;。相同的key，就会merge成一个group{aaa，[5, 8…]}，这个数组中的不同的值，就是从不同的溢写文件中读取过来的，然后把这些值加起来。==</p>
<p><strong>为什么要设置内存缓冲区：</strong>批量收集 map 的结果，减少磁盘 IO 的次数，提高频率。</p>
<p><strong>磁盘文件要写到哪里：</strong>写磁盘将按照轮询方式写到mapred.local.dir属性指定的作业特定子目录的目录中。也就是存放在TaskTracker够得着的某个本地目录，每一个reduce task不断通过RPC从JobTracker中获取map task是否完成的信息，如果reduce task得到通知，获知某台TaskTracker上的map task完成，Shuffle的后半段就开始了。 </p>
<p><strong>所有的合并究竟是为什么：</strong> 因为map节点和reduce节点之间的数据拷贝是通过网络进行拷贝的，数据量越小，拷贝的越快，相应的处理也就越快。所以合并的目的就是减少map的输出数据量，让网络拷贝尽可能快。</p>
<p>需要特殊说明的是，以上的步骤，都是在本地机器上完成，并不需要通过网络进行数据的传输。   </p>
<h4 id="reduce的shuffle细节"><a href="#reduce的shuffle细节" class="headerlink" title="reduce的shuffle细节"></a>reduce的shuffle细节</h4><p><strong>copy阶段</strong></p>
<p>reduce进程启动一些数据的copy线程，这个线程叫做fetcher线程，通过http方式请求map task所在的TaskTracker，来获取map task的输出数据。</p>
<p>reduce拷贝数据，不是进行随意的拷贝，之前的partition，已经将数据分好区，reduce只是拷贝各个map上分割给自己的那一部分数据，拷贝到本地后，从每一个map上拷贝过来的数据都是一个小文件，也是需要对这些小文件进行合并的。合并以后，输出到reduce进行处理。</p>
<ul>
<li><p>加入client有设置过combiner，那么现在就是使用combiner的时候了。将有相同key的key/value对的value加起来，减少溢写到磁盘的数据量。</p>
</li>
<li><p>当整个map结束后再对磁盘中这个map所产生的所有临时文件做合并（merge）</p>
</li>
<li><p>reduce从tasktracker copy数据</p>
</li>
<li><p>copy 过来的数据会先放在内存缓冲区，这里的缓冲区大小要比map端的更为灵活，它基于jvm的heap size 设置</p>
</li>
<li><p>merge的三种形式：1、内存到内存；2、内存到磁盘；3、磁盘到磁盘；merge从不同tasktracker上拿到的数据。</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/MapReduce%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/MapReduce%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">MapReduce源码分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:58:33" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/Spark%20SQL%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/Spark%20SQL%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Spark SQL原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:55:23" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Spark-SQL原理"><a href="#Spark-SQL原理" class="headerlink" title="Spark SQL原理"></a>Spark SQL原理</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点：</p>
<ul>
<li>能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询；</li>
<li>支持多种开发语言；</li>
<li>支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等；</li>
<li>支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库；</li>
<li>支持标准的 JDBC 和 ODBC 连接；</li>
<li>支持优化器，列式存储和代码生成等特性；</li>
<li>支持扩展并能保证容错。</li>
</ul>
<h2 id="DataFrame和RDD"><a href="#DataFrame和RDD" class="headerlink" title="DataFrame和RDD"></a>DataFrame和RDD</h2><p>主要区别在于RDD面向的是非结构化数据，DataFreame面向的是结构化数据。</p>
<p>DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。</p>
<h2 id="DataFrame-和-RDDs-应该如何选择？"><a href="#DataFrame-和-RDDs-应该如何选择？" class="headerlink" title="DataFrame 和 RDDs 应该如何选择？"></a>DataFrame 和 RDDs 应该如何选择？</h2><ul>
<li>如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs；</li>
<li>如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs，</li>
<li>如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。</li>
</ul>
<h2 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h2><p>DataSet 也是分布式的数据集合，在Spark1.6版本引入，它继承了RDD和DataFrame的优点，具备强类型的特点，同时支持Lambda函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。</p>
<p>静态类型与运行时类型安全</p>
<p>静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现如下:</p>
<p>在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于：</p>
<p>在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。</p>
<p>以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。</p>
<h2 id="Untypeed-和-Typed"><a href="#Untypeed-和-Typed" class="headerlink" title="Untypeed 和 Typed"></a>Untypeed 和 Typed</h2><p>DataFrame API 被标记为 Untyped API，而 DataSet API 被标记为 Typed API。DataFrame 的 Untyped 是相对于语言或 API 层面而言，它确实有明确的 Scheme 结构，即列名，列类型都是确定的，但这些信息完全由 Spark 来维护，Spark 只会在运行时检查这些类型和指定类型是否一致。这也就是为什么在 Spark 2.0 之后，官方推荐把 DataFrame 看做是 DatSet[Row]，Row 是 Spark 中定义的一个 trait，其子类中封装了列字段的信息。</p>
<p>相对而言，DataSet 是 Typed 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 Person，这些信息由 JVM 来保证正确性，所以字段名错误和类型错误在编译的时候就会被 IDE 所发现。</p>
<p>val dataSet: Dataset[Person] = spark.read.json(“people.json”).as[Person]</p>
<h2 id="DataFrame-amp-DataSet-amp-RDDs-总结"><a href="#DataFrame-amp-DataSet-amp-RDDs-总结" class="headerlink" title="DataFrame &amp; DataSet &amp; RDDs 总结"></a>DataFrame &amp; DataSet &amp; RDDs 总结</h2><ul>
<li>RDDs 适合非结构化数据的处理，而 DataFrame &amp; DataSet 更适合结构化数据和半结构化的处理；</li>
<li>DataFrame &amp; DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景；</li>
<li>相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查；</li>
<li>DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。</li>
</ul>
<h2 id="运行原理"><a href="#运行原理" class="headerlink" title="运行原理"></a>运行原理</h2><p>DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的：</p>
<ol>
<li>进行 DataFrame/Dataset/SQL 编程；</li>
<li>如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划；</li>
<li>Spark 将此逻辑计划转换为物理计划，同时进行代码优化；</li>
<li>Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/11/Spark%20SQL%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/11/Spark%20SQL%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">Spark源码</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-11 12:22:00 / 修改时间：14:55:29" itemprop="dateCreated datePublished" datetime="2020-12-11T12:22:00+08:00">2020-12-11</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/bigdata/" itemprop="url" rel="index"><span itemprop="name">bigdata</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Spark-SQL源码分析"><a href="#Spark-SQL源码分析" class="headerlink" title="Spark SQL源码分析"></a>Spark SQL源码分析</h1><p><strong>调用SQL时</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">SparkSession</span> -&gt;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sql</span></span>(sqlText: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line"></span><br><span class="line"> <span class="type">Dataset</span>.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Parser"><a href="#Parser" class="headerlink" title="Parser"></a>Parser</h2><p>parsePlan方法会返回一个LogicalPlan对象；</p>
<p>第一步，利用 antlr4 生成的 SqlBaseLexer【val lexer = new SqlBaseLexer(new UpperCaseCharStream(CharStreams.fromString(command)))】 对SQL进行词法分析，生成一个CommonTokenStream 对象【val tokenStream = new CommonTokenStream(lexer)】</p>
<p>第二步，利用 antlr4 生成的 SqlBaseParser 【val parser = new SqlBaseParser(tokenStream)】对SQL进行语法分析，得到 Unresolved LogicalPlan</p>
<p>以下均在 QueryExecution 中执行</p>
<h2 id="Analyzer"><a href="#Analyzer" class="headerlink" title="Analyzer"></a>Analyzer</h2><p>Analyzer 持有一个 SessionCatalog 对象的引用</p>
<p>Analyzer 继承自 RuleExecutor[LogicalPlan]，因此可以对 LogicalPlan 进行转换</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> analyzed: <span class="type">LogicalPlan</span> = &#123;</span><br><span class="line"></span><br><span class="line"> <span class="type">SparkSession</span>.setActiveSession(sparkSession)</span><br><span class="line"></span><br><span class="line"> sparkSession.sessionState.analyzer.executeAndCheck(logical)</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过 Catalog 确定每张表对应的字段集、字段类型、数据存储位置，生成Resolved Logical Plan</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkAnalysis</span></span>(plan: <span class="type">LogicalPlan</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">	<span class="keyword">case</span> u: <span class="type">UnresolvedRelation</span> =&gt;</span><br><span class="line"></span><br><span class="line">	 u.failAnalysis(<span class="string">s&quot;Table or view not found: <span class="subst">$&#123;u.tableIdentifier&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>ResolveRelations</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 关联表</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">lookupTableFromCatalog</span></span>(</span><br><span class="line"></span><br><span class="line">  u: <span class="type">UnresolvedRelation</span>,</span><br><span class="line"></span><br><span class="line">  defaultDatabase: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span>): <span class="type">LogicalPlan</span> =&#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> optimizedPlan: <span class="type">LogicalPlan</span> = sparkSession.sessionState.optimizer.execute(withCachedData)</span><br></pre></td></tr></table></figure>

<p>逻辑优化器，会进行谓词下推，列值裁剪，常量折叠，谓词合并等等一系列逻辑优化</p>
<p>根据预先定义好的规则对 Resolved Logical Plan 进行优化并生成 Optimized Logical Plan</p>
<h2 id="SparkPlanner"><a href="#SparkPlanner" class="headerlink" title="SparkPlanner"></a>SparkPlanner</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> sparkPlan: <span class="type">SparkPlan</span> = &#123;</span><br><span class="line"></span><br><span class="line"> <span class="type">SparkSession</span>.setActiveSession(sparkSession)</span><br><span class="line"></span><br><span class="line"> <span class="comment">// <span class="doctag">TODO:</span> We use next(), i.e. take the first plan returned by the planner, here for now,</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">//    but we will implement to choose the best plan.</span></span><br><span class="line"></span><br><span class="line"> planner.plan(<span class="type">ReturnAnswer</span>(optimizedPlan)).next()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>把 Logical Plan 转变为 Physical Plan</p>
<h2 id="执行-Physical-Plan"><a href="#执行-Physical-Plan" class="headerlink" title="执行 Physical Plan"></a>执行 Physical Plan</h2><p>lazy val executedPlan: SparkPlan = prepareForExecution(sparkPlan)</p>
<p><strong>转成RDD</strong></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Internal version of the RDD. Avoids copies and has no schema */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">lazy</span> <span class="keyword">val</span> toRdd: <span class="type">RDD</span>[<span class="type">InternalRow</span>] = &#123;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> (sparkSession.sessionState.conf.getConf(<span class="type">SQLConf</span>.<span class="type">USE_CONF_ON_RDD_OPERATION</span>)) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">new</span> <span class="type">SQLExecutionRDD</span>(executedPlan.execute(), sparkSession.sessionState.conf)</span><br><span class="line"></span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line"></span><br><span class="line">  executedPlan.execute()</span><br><span class="line"></span><br><span class="line"> &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>






<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sevncz</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  







  






</body>
</html>
