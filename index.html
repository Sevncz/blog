<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto+Slab:300,300italic,400,400italic,700,700italic|PT+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sevncz.xyz","root":"/","images":"/images","scheme":"Pisces","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="努力赚钱">
<meta property="og:type" content="website">
<meta property="og:title" content="Sevncz&#39;s Blog">
<meta property="og:url" content="http://sevncz.xyz/index.html">
<meta property="og:site_name" content="Sevncz&#39;s Blog">
<meta property="og:description" content="努力赚钱">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sevncz">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://sevncz.xyz/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>
<title>Sevncz's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Sevncz's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="sevncz"
      src="/images/IMG_1169.JPG">
  <p class="site-author-name" itemprop="name">sevncz</p>
  <div class="site-description" itemprop="description">努力赚钱</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">4</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">19</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Sevncz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sevncz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2021/01/18/Spark-Streaming-%E5%8F%8D%E5%8E%8B%E9%99%90%E6%B5%81%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E6%B5%81%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2021/01/18/Spark-Streaming-%E5%8F%8D%E5%8E%8B%E9%99%90%E6%B5%81%E5%AE%9E%E7%8E%B0%E6%BA%90%E7%A0%81%E6%B5%81%E7%A8%8B/" class="post-title-link" itemprop="url">Spark Streaming 反压限流实现源码流程</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2021-01-18 18:07:58 / 修改时间：18:18:32" itemprop="dateCreated datePublished" datetime="2021-01-18T18:07:58+08:00">2021-01-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="1-RateController"><a href="#1-RateController" class="headerlink" title="1. RateController"></a>1. RateController</h2><p><code>RateController extend StreamingListener</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 这个注册在StreamingListenerBus监听器中 </span></span><br><span class="line"><span class="comment">// StreamingListenerBus extend SparkListener with ListenerBus[StreamingListener, StreamingListenerEvent]</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doPostEvent</span></span>(</span><br><span class="line">      listener: <span class="type">StreamingListener</span>,</span><br><span class="line">      event: <span class="type">StreamingListenerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    event <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> receiverStarted: <span class="type">StreamingListenerReceiverStarted</span> =&gt;</span><br><span class="line">        listener.onReceiverStarted(receiverStarted)</span><br><span class="line">      <span class="keyword">case</span> receiverError: <span class="type">StreamingListenerReceiverError</span> =&gt;</span><br><span class="line">        listener.onReceiverError(receiverError)</span><br><span class="line">      <span class="keyword">case</span> receiverStopped: <span class="type">StreamingListenerReceiverStopped</span> =&gt;</span><br><span class="line">        listener.onReceiverStopped(receiverStopped)</span><br><span class="line">      <span class="keyword">case</span> batchSubmitted: <span class="type">StreamingListenerBatchSubmitted</span> =&gt;</span><br><span class="line">        listener.onBatchSubmitted(batchSubmitted)</span><br><span class="line">      <span class="keyword">case</span> batchStarted: <span class="type">StreamingListenerBatchStarted</span> =&gt;</span><br><span class="line">        listener.onBatchStarted(batchStarted)</span><br><span class="line">      <span class="comment">// 注册监听器</span></span><br><span class="line">      <span class="keyword">case</span> batchCompleted: <span class="type">StreamingListenerBatchCompleted</span> =&gt;</span><br><span class="line">        listener.onBatchCompleted(batchCompleted)</span><br><span class="line">      <span class="keyword">case</span> outputOperationStarted: <span class="type">StreamingListenerOutputOperationStarted</span> =&gt;</span><br><span class="line">        listener.onOutputOperationStarted(outputOperationStarted)</span><br><span class="line">      <span class="keyword">case</span> outputOperationCompleted: <span class="type">StreamingListenerOutputOperationCompleted</span> =&gt;</span><br><span class="line">        listener.onOutputOperationCompleted(outputOperationCompleted)</span><br><span class="line">      <span class="keyword">case</span> streamingStarted: <span class="type">StreamingListenerStreamingStarted</span> =&gt;</span><br><span class="line">        listener.onStreamingStarted(streamingStarted)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>) &#123;</span><br><span class="line"> <span class="keyword">val</span> elements = batchCompleted.batchInfo.streamIdToInputInfo</span><br><span class="line"> <span class="keyword">for</span> &#123;</span><br><span class="line">  <span class="comment">// 获取批次执行的基本信息，最后完成时间、处理延迟、调度延迟、以及StreamID与记录数的对应值</span></span><br><span class="line">  processingEnd &lt;- batchCompleted.batchInfo.processingEndTime</span><br><span class="line">  workDelay &lt;- batchCompleted.batchInfo.processingDelay</span><br><span class="line">  waitDelay &lt;- batchCompleted.batchInfo.schedulingDelay</span><br><span class="line">  elems &lt;- elements.get(streamUID).map(_.numRecords)</span><br><span class="line"> &#125; computeAndPublish(processingEnd, elems, workDelay, waitDelay)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算新的速率，由rateEstimator.compute计算</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">computeAndPublish</span></span>(time: <span class="type">Long</span>, elems: <span class="type">Long</span>, workDelay: <span class="type">Long</span>, waitDelay: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line"> <span class="type">Future</span>[<span class="type">Unit</span>] &#123;</span><br><span class="line">  <span class="keyword">val</span> newRate = rateEstimator.compute(time, elems, workDelay, waitDelay)</span><br><span class="line">  newRate.foreach &#123; s =&gt;</span><br><span class="line">   rateLimit.set(s.toLong)</span><br><span class="line">   publish(getLatestRate())</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-RateEstimator-的一个实现-PIDRateEstimator"><a href="#2-RateEstimator-的一个实现-PIDRateEstimator" class="headerlink" title="2. RateEstimator 的一个实现 PIDRateEstimator"></a><strong>2. RateEstimator</strong> 的一个实现 <strong>PIDRateEstimator</strong></h2><p>PIDRateEstimator 实现了 RateEstimator 的 compute 方法，其计算过程主要运用到经典的工程学控制算法 PID，这是一种通过误差的比例、积分、微分共同作用的反馈控制算法，能够很好的通过误差反馈实现比较有利那个的目标量的调节。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute</span></span>(</span><br><span class="line">      time: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      numElements: <span class="type">Long</span>,</span><br><span class="line">      processingDelay: <span class="type">Long</span>, <span class="comment">// in milliseconds</span></span><br><span class="line">      schedulingDelay: <span class="type">Long</span> <span class="comment">// in milliseconds</span></span><br><span class="line">    ): <span class="type">Option</span>[<span class="type">Double</span>] = &#123;</span><br><span class="line">    logTrace(<span class="string">s&quot;\ntime = <span class="subst">$time</span>, # records = <span class="subst">$numElements</span>, &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;processing time = <span class="subst">$processingDelay</span>, scheduling delay = <span class="subst">$schedulingDelay</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">if</span> (time &gt; latestTime &amp;&amp; numElements &gt; <span class="number">0</span> &amp;&amp; processingDelay &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in seconds, should be close to batchDuration</span></span><br><span class="line">        <span class="keyword">val</span> delaySinceUpdate = (time - latestTime).toDouble / <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> processingRate = numElements.toDouble / processingDelay * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">// In our system `error` is the difference between the desired rate and the measured rate</span></span><br><span class="line">        <span class="comment">// based on the latest batch information. We consider the desired rate to be latest rate,</span></span><br><span class="line">        <span class="comment">// which is what this estimator calculated for the previous batch.</span></span><br><span class="line">        <span class="comment">// in elements/second</span></span><br><span class="line">        <span class="keyword">val</span> error = latestRate - processingRate</span><br><span class="line"></span><br><span class="line">        <span class="comment">// The error integral, based on schedulingDelay as an indicator for accumulated errors.</span></span><br><span class="line">        <span class="comment">// A scheduling delay s corresponds to s * processingRate overflowing elements. Those</span></span><br><span class="line">        <span class="comment">// are elements that couldn&#x27;t be processed in previous batches, leading to this delay.</span></span><br><span class="line">        <span class="comment">// In the following, we assume the processingRate didn&#x27;t change too much.</span></span><br><span class="line">        <span class="comment">// From the number of overflowing elements we can calculate the rate at which they would be</span></span><br><span class="line">        <span class="comment">// processed by dividing it by the batch interval. This rate is our &quot;historical&quot; error,</span></span><br><span class="line">        <span class="comment">// or integral part, since if we subtracted this rate from the previous &quot;calculated rate&quot;,</span></span><br><span class="line">        <span class="comment">// there wouldn&#x27;t have been any overflowing elements, and the scheduling delay would have</span></span><br><span class="line">        <span class="comment">// been zero.</span></span><br><span class="line">        <span class="comment">// (in elements/second)</span></span><br><span class="line">        <span class="keyword">val</span> historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis</span><br><span class="line"></span><br><span class="line">        <span class="comment">// in elements/(second ^ 2)</span></span><br><span class="line">        <span class="keyword">val</span> dError = (error - latestError) / delaySinceUpdate</span><br><span class="line">        <span class="comment">// 这里是具体算法，将计算值和通过spark.streaming.backpressure.pid.minRate参数设置的值取大者作为下一次输入值</span></span><br><span class="line">        <span class="keyword">val</span> newRate = (latestRate - proportional * error -</span><br><span class="line">                                    integral * historicalError -</span><br><span class="line">                                    derivative * dError).max(minRate)</span><br><span class="line">        logTrace(<span class="string">s&quot;&quot;</span><span class="string">&quot;</span></span><br><span class="line"><span class="string">            | latestRate = $latestRate, error = $error</span></span><br><span class="line"><span class="string">            | latestError = $latestError, historicalError = $historicalError</span></span><br><span class="line"><span class="string">            | delaySinceUpdate = $delaySinceUpdate, dError = $dError</span></span><br><span class="line"><span class="string">            &quot;</span><span class="string">&quot;&quot;</span>.stripMargin)</span><br><span class="line"></span><br><span class="line">        latestTime = time</span><br><span class="line">        <span class="keyword">if</span> (firstRun) &#123;</span><br><span class="line">          latestRate = processingRate</span><br><span class="line">          latestError = <span class="number">0</span>D</span><br><span class="line">          firstRun = <span class="literal">false</span></span><br><span class="line">          logTrace(<span class="string">&quot;First run, rate estimation skipped&quot;</span>)</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          latestRate = newRate</span><br><span class="line">          latestError = error</span><br><span class="line">          logTrace(<span class="string">s&quot;New rate = <span class="subst">$newRate</span>&quot;</span>)</span><br><span class="line">          <span class="type">Some</span>(newRate)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logTrace(<span class="string">&quot;Rate estimation skipped&quot;</span>)</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="3-PUSH"><a href="#3-PUSH" class="headerlink" title="3. PUSH"></a><strong>3. PUSH</strong></h2><p>ReceiverInputDStream 是 RateController 的一个实现子类，实现了父类的 publish 方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[streaming] <span class="class"><span class="keyword">class</span> <span class="title">ReceiverRateController</span>(<span class="params">id: <span class="type">Int</span>, estimator: <span class="type">RateEstimator</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">RateController</span>(<span class="params">id, estimator</span>) </span>&#123;</span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">publish</span></span>(rate: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line">  ssc.scheduler.receiverTracker.sendRateUpdate(id, rate)</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这是具体的send过程，endpoint实际为ReceiverTrackerEndPoint，这里能够看出用的是Spark的RPC通信机制，通过在RpcEnv中注册的ReceiverTrackerEndpoint将控制信息发给Receiver。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sendRateUpdate</span></span>(streamUID: <span class="type">Int</span>, newRate: <span class="type">Long</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line"> <span class="keyword">if</span> (isTrackerStarted) &#123;</span><br><span class="line">  endpoint.send(<span class="type">UpdateReceiverRateLimit</span>(streamUID, newRate))</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后再发送到Receiver注册的RpcEndpoint，用于接收Driver中ReceiverTracker发送过来的消息并处理。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">UpdateReceiverRateLimit</span>(streamUID, newRate) =&gt;</span><br><span class="line"> <span class="keyword">for</span> (info &lt;- receiverTrackingInfos.get(streamUID); eP &lt;- info.endpoint) &#123;</span><br><span class="line">  eP.send(<span class="type">UpdateRateLimit</span>(newRate))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>端点接收消息后匹配处理，在ReceiverSupervisorImpl中</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">UpdateRateLimit</span>(eps) =&gt;</span><br><span class="line"> logInfo(<span class="string">s&quot;Received a new rate limit: <span class="subst">$eps</span>.&quot;</span>)</span><br><span class="line"> registeredBlockGenerators.asScala.foreach &#123; bg =&gt;</span><br><span class="line">  bg.updateRate(eps)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<h2 id="4-RateLimiter-具体速率限制的类"><a href="#4-RateLimiter-具体速率限制的类" class="headerlink" title="4. RateLimiter 具体速率限制的类"></a><strong>4. RateLimiter</strong> <strong>具体速率限制的类</strong></h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[receiver] <span class="function"><span class="keyword">def</span> <span class="title">updateRate</span></span>(newRate: <span class="type">Long</span>): <span class="type">Unit</span> =</span><br><span class="line"> <span class="keyword">if</span> (newRate &gt; <span class="number">0</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (maxRateLimit &gt; <span class="number">0</span>) &#123;</span><br><span class="line">   rateLimiter.setRate(newRate.min(maxRateLimit))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">   rateLimiter.setRate(newRate)</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到由Guava的实现库RateLimiter实现：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// treated as an upper limit</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> maxRateLimit = conf.getLong(<span class="string">&quot;spark.streaming.receiver.maxRate&quot;</span>, <span class="type">Long</span>.<span class="type">MaxValue</span>)</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> rateLimiter = <span class="type">GuavaRateLimiter</span>.create(getInitialRateLimit().toDouble)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">waitToPush</span></span>() &#123;</span><br><span class="line"> <span class="comment">// 获取许可，如获取不到则阻塞</span></span><br><span class="line"> rateLimiter.acquire()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用RateLimiter的地方，BlockGenerator</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Push a single data item into the buffer.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addData</span></span>(data: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line"> <span class="keyword">if</span> (state == <span class="type">Active</span>) &#123;</span><br><span class="line">  <span class="comment">// 这里就会限制速率</span></span><br><span class="line">  waitToPush()</span><br><span class="line">  synchronized &#123;</span><br><span class="line">   <span class="keyword">if</span> (state == <span class="type">Active</span>) &#123;</span><br><span class="line">    currentBuffer += data</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">     <span class="string">&quot;Cannot add data as BlockGenerator has not been started or has been stopped&quot;</span>)</span><br><span class="line">   &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">   <span class="string">&quot;Cannot add data as BlockGenerator has not been started or has been stopped&quot;</span>)</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/23/Redis-Docker%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/23/Redis-Docker%E9%9B%86%E7%BE%A4%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" class="post-title-link" itemprop="url">Redis Docker集群测试环境搭建</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-23 13:50:32 / 修改时间：14:02:16" itemprop="dateCreated datePublished" datetime="2020-12-23T13:50:32+08:00">2020-12-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/Mac/" itemprop="url" rel="index"><span itemprop="name">Mac</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="基本环境说明"><a href="#基本环境说明" class="headerlink" title="基本环境说明"></a>基本环境说明</h2><ol>
<li><p>macOS 10.14.6</p>
</li>
<li><p>Docker Desktop 3.03(51017)</p>
<ul>
<li>Engine  20.10.0</li>
<li>Compose 1.27.4</li>
</ul>
</li>
<li><p>images</p>
<ul>
<li>redis 5.0.8</li>
</ul>
</li>
</ol>
<h2 id="Docker-配置"><a href="#Docker-配置" class="headerlink" title="Docker 配置"></a>Docker 配置</h2><h3 id="dockerfile"><a href="#dockerfile" class="headerlink" title="dockerfile"></a>dockerfile</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM redis:5.0.8</span><br><span class="line">MAINTAINER Wen &lt;wen0112@live.com.com&gt;</span><br><span class="line">COPY rediscluster.conf &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf</span><br><span class="line">ENTRYPOINT redis-server &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf</span><br></pre></td></tr></table></figure>

<h3 id="rediscluster-conf"><a href="#rediscluster-conf" class="headerlink" title="rediscluster.conf"></a>rediscluster.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># ip</span><br><span class="line">bind 0.0.0.0</span><br><span class="line"># 启动 cluster</span><br><span class="line">cluster-enabled yes</span><br><span class="line"># 指定 cluster config</span><br><span class="line">cluster-config-file nodes.conf</span><br><span class="line"># 指定 node 无法连接时间</span><br><span class="line">cluster-node-timeout 5000</span><br><span class="line">#设置主服务器连接密码</span><br><span class="line">masterauth pass.123</span><br><span class="line">#设置从服务器连接密码</span><br><span class="line">requirepass pass.123</span><br></pre></td></tr></table></figure>

<h3 id="docker-compose"><a href="#docker-compose" class="headerlink" title="docker-compose"></a>docker-compose</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;3.4&#39;</span><br><span class="line"></span><br><span class="line">services:</span><br><span class="line">  redis-node1:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7000:7000&quot;</span><br><span class="line">      - &quot;17000:17000&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7000&quot;, --cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">  redis-node2:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7001:7001&quot;</span><br><span class="line">      - &quot;17001:17001&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7001&quot;,--cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">  redis-node3:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7002:7002&quot;</span><br><span class="line">      - &quot;17002:17002&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7002&quot;,--cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">  redis-node4:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7003:7003&quot;</span><br><span class="line">      - &quot;17003:17003&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7003&quot;,--cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - redis-node1</span><br><span class="line">      - redis-node2</span><br><span class="line">      - redis-node3</span><br><span class="line">  redis-node5:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7004:7004&quot;</span><br><span class="line">      - &quot;17004:17004&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7004&quot;,--cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - redis-node1</span><br><span class="line">      - redis-node2</span><br><span class="line">      - redis-node3</span><br><span class="line">  redis-node6:</span><br><span class="line">    build:</span><br><span class="line">      context: redis</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;7005:7005&quot;</span><br><span class="line">      - &quot;17005:17005&quot;</span><br><span class="line">    restart: always</span><br><span class="line">    entrypoint: [redis-server, &#x2F;etc&#x2F;redis&#x2F;rediscluster.conf,    --port,&quot;7005&quot;,--cluster-announce-ip,&quot;$&#123;ip&#125;&quot;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - redis-node1</span><br><span class="line">      - redis-node2</span><br><span class="line">      - redis-node3</span><br><span class="line">  redis-cluster-creator:</span><br><span class="line">    image: redis:5.0.8</span><br><span class="line">    entrypoint: [&#x2F;bin&#x2F;sh,-c,&#39;echo &quot;yes&quot; | redis-cli -a pass.123 --cluster create $&#123;ip&#125;:7000 $&#123;ip&#125;:7001 $&#123;ip&#125;:7002 $&#123;ip&#125;:7003 $&#123;ip&#125;:7004 $&#123;ip&#125;:7005 --cluster-replicas 1&#39;]</span><br><span class="line">    depends_on:</span><br><span class="line">      - redis-node1</span><br><span class="line">      - redis-node2</span><br><span class="line">      - redis-node3</span><br><span class="line">      - redis-node4</span><br><span class="line">      - redis-node5</span><br><span class="line">      - redis-node6</span><br></pre></td></tr></table></figure>

<h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ip=$(ipconfig getifaddr en0) docker-compose up -d --build</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/" class="post-title-link" itemprop="url">Redis实现分布式锁</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-22 22:38:39 / 修改时间：23:17:28" itemprop="dateCreated datePublished" datetime="2020-12-22T22:38:39+08:00">2020-12-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>和单机上的锁类似，分布式锁同样可以用一个变量来实现。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为 0，表明客户端不再持有锁。</p>
<p>在分布式场景下，锁变量需要由一个共享存储系统来维护，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值。</p>
<h2 id="分布式锁的两个要求"><a href="#分布式锁的两个要求" class="headerlink" title="分布式锁的两个要求"></a>分布式锁的两个要求</h2><ul>
<li><p>要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性；</p>
</li>
<li><p>要求二：共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。</p>
</li>
</ul>
<p>我们既可以基于单个 Redis 节点来实现，也可以使用多个 Redis 节点实现。在这两种情况下，锁的可靠性是不一样的。</p>
<h2 id="基于单个-Redis-节点实现分布式锁"><a href="#基于单个-Redis-节点实现分布式锁" class="headerlink" title="基于单个 Redis 节点实现分布式锁"></a>基于单个 Redis 节点实现分布式锁</h2><p>加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为 1），而这三个操作在执行时需要保证原子性。那怎么保证原子性呢？</p>
<h3 id="SETNX-命令"><a href="#SETNX-命令" class="headerlink" title="SETNX 命令"></a>SETNX 命令</h3><p>它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETNX key value</span><br></pre></td></tr></table></figure>

<p>我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加锁</span></span><br><span class="line">SETNX lock_key <span class="number">1</span></span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line">DO THINGS</span><br><span class="line"><span class="comment">// 释放锁</span></span><br><span class="line">DEL lock_key</span><br></pre></td></tr></table></figure>

<p>不过，使用 SETNX 和 DEL 命令组合实现分布锁，存在两个潜在的风险。</p>
<p>第一个风险是，加锁之后的操作发生异常，无法执行 DEL 命令释放锁，因此别的客户端也无法拿到锁，无法执行后续操作。针对这个问题，可以<strong>给锁变量加一个过期时间</strong>。</p>
<p>第二个风险是，客户端A执行了 SETNX 命令加锁后，客户端B执行了DEL命令释放锁，此时，客户端A的锁就被误释放了。针对这个问题，我们需要能<strong>区分来自不同客户端的锁操作</strong>。在使用 SETNX 命令进行加锁的方法中，我们通过把锁变量值设置为 1 或 0，表示是否加锁成功。1 和 0 只有两种状态，无法表示究竟是哪个客户端进行的锁操作。所以，我们在加锁操作时，可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题了。</p>
<h2 id="基于多个-Redis-节点实现高可靠的分布式锁"><a href="#基于多个-Redis-节点实现高可靠的分布式锁" class="headerlink" title="基于多个 Redis 节点实现高可靠的分布式锁"></a>基于多个 Redis 节点实现高可靠的分布式锁</h2><p>当我们要实现高可靠的分布式锁时，就不能依赖单个的命令操作了，我们需要安装一定的步骤和规则进行加解锁操作。否则，就可能会出现锁无法工作的情况。“一定的步骤和规则”是指啥呢？其实就是分布式锁的算法。</p>
<p>为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 <strong>Redlock</strong>。</p>
<p>Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例一次请求加锁，如果客户端能够和半数以上的实例成功完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。</p>
<p>Redlock 算法的实现需要有 N 个独立的 Redis 实例。以下是加锁步骤：</p>
<ol>
<li><p><strong>客户端获取当前时间；</strong></p>
</li>
<li><p><strong>客户端按顺序依次向 N 个 Redis 实例执行加锁操作；</strong></p>
<p>这里的加锁操作和在单实例上执行的加锁操作一样，使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。</p>
<p>如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。</p>
</li>
<li><p><strong>一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。</strong></p>
</li>
</ol>
<p>客户端只有在满足下面的这两个条件时，才能认为是加锁成功。</p>
<ul>
<li><p><strong>条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁；</strong></p>
</li>
<li><p><strong>条件二：客户端获取锁的总耗时没有超过锁的有效时间。</strong></p>
</li>
</ul>
<p>在满足了这两个条件后，我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。</p>
<p>如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。</p>
<p>在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Redis-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Redis-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Redis 主从数据同步原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-22 17:11:53 / 修改时间：17:12:02" itemprop="dateCreated datePublished" datetime="2020-12-22T17:11:53+08:00">2020-12-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Spark ShuffleManager的运行原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-22 15:01:22" itemprop="dateCreated datePublished" datetime="2020-12-22T15:01:22+08:00">2020-12-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-01-19 12:14:26" itemprop="dateModified" datetime="2021-01-19T12:14:26+08:00">2021-01-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h2><p>有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。</p>
<h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p>先看下SortShuffleManager的官方说明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * In sort-based shuffle, incoming records are sorted according to their target partition ids, then</span></span><br><span class="line"><span class="comment"> * written to a single map output file. Reducers fetch contiguous regions of this file in order to</span></span><br><span class="line"><span class="comment"> * read their portion of the map output. In cases where the map output data is too large to fit in</span></span><br><span class="line"><span class="comment"> * memory, sorted subsets of the output can be spilled to disk and those on-disk files are merged</span></span><br><span class="line"><span class="comment"> * to produce the final output file.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> # 有两种不同的方式输出文件</span></span><br><span class="line"><span class="comment"> * Sort-based shuffle has two different write paths for producing its map output files:</span></span><br><span class="line"><span class="comment"> # 序列化排序，当一下三个条件都成立时使用</span></span><br><span class="line"><span class="comment"> *  - Serialized sorting: used when all three of the following conditions hold:</span></span><br><span class="line"><span class="comment"> # shuffle 的依赖没有指定的聚合或输出排序</span></span><br><span class="line"><span class="comment"> *    1. The shuffle dependency specifies no aggregation or output ordering.</span></span><br><span class="line"><span class="comment"> # shuffle 的序列化支持序列化值的重定位</span></span><br><span class="line"><span class="comment"> *    2. The shuffle serializer supports relocation of serialized values (this is currently supported by KryoSerializer and Spark SQL&#x27;s custom serializers).</span></span><br><span class="line"><span class="comment"> # shuffle 产生的输出分区少于16777216</span></span><br><span class="line"><span class="comment"> *    3. The shuffle produces fewer than 16777216 output partitions.</span></span><br><span class="line"><span class="comment"> *  - Deserialized sorting: used to handle all other cases.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> * Serialized sorting mode</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 优化手段</span></span><br><span class="line"><span class="comment"> * In the serialized sorting mode, incoming records are serialized as soon as they are passed to the</span></span><br><span class="line"><span class="comment"> * shuffle writer and are buffered in a serialized form during sorting. This write path implements</span></span><br><span class="line"><span class="comment"> * several optimizations:</span></span><br><span class="line"><span class="comment"> # 排序在二进制数据上而不是Java对象</span></span><br><span class="line"><span class="comment"> *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory</span></span><br><span class="line"><span class="comment"> *    consumption and GC overheads. This optimization requires the record serializer to have certain</span></span><br><span class="line"><span class="comment"> *    properties to allow serialized records to be re-ordered without requiring deserialization.</span></span><br><span class="line"><span class="comment"> *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 使用一个专门的缓存效率高的分拣器([[ShuffleExternalSorter]])，对压缩记录指针和分区ID的数组进行分拣。通过在排序数组中每条记录只使用8个字节的空间，这可以将更多的数组放入缓存。</span></span><br><span class="line"><span class="comment"> *  - It uses a specialized cache-efficient sorter ([[ShuffleExternalSorter]]) that sorts</span></span><br><span class="line"><span class="comment"> *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per</span></span><br><span class="line"><span class="comment"> *    record in the sorting array, this fits more of the array into cache.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 溢出合并程序对属于同一分区的序列化记录块进行操作，在合并过程中不需要反序列化记录。</span></span><br><span class="line"><span class="comment"> *  - The spill merging procedure operates on blocks of serialized records that belong to the same</span></span><br><span class="line"><span class="comment"> *    partition and does not need to deserialize records during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> #  当spill压缩编解码器支持压缩数据的连接时，spill合并只是将序列化和压缩的spill分区连接起来，产生最终的输出分区。 这样就可以使用高效的数据复制方法，如NIO的 &quot;transferTo&quot;，并避免了在合并过程中分配解压或复制缓冲区的需要。</span></span><br><span class="line"><span class="comment"> *  - When the spill compression codec supports concatenation of compressed data, the spill merge</span></span><br><span class="line"><span class="comment"> *    simply concatenates the serialized and compressed spill partitions to produce the final output</span></span><br><span class="line"><span class="comment"> *    partition.  This allows efficient data copying methods, like NIO&#x27;s `transferTo`, to be used</span></span><br><span class="line"><span class="comment"> *    and avoids the need to allocate decompression or copying buffers during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For more details on these optimizations, see SPARK-7081.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>

<p>主要方法</p>
<ul>
<li>registerShuffle</li>
<li>getReader</li>
<li>getWriter</li>
<li>unregisterShuffle</li>
</ul>
<h4 id="三种-writer-说明及选择时机"><a href="#三种-writer-说明及选择时机" class="headerlink" title="三种 writer 说明及选择时机"></a>三种 writer 说明及选择时机</h4><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>和Hash Shuffle实现基本相同，区别在于map task输出会汇总为一个文件</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>tungsten-sort，ShuffleExternalSorter使用Java Unsafe直接操作内存，避免Java对象多余的开销和GC 延迟，效率高</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>Sort Shuffle，和Hash Shuffle的主要不同在于，map端支持Partition级别的sort，map task输出会汇总为一个文件</td>
</tr>
</tbody></table>
<p>Spark根据运行时信息选择三种ShuffleWriter实现中的一种，对应的源码为<strong>SortShuffleManager</strong>中的<strong>registerShuffle</strong>方法，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Obtains a [[ShuffleHandle]] to pass to tasks.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      numMaps: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="comment">// If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#x27;t</span></span><br><span class="line">      <span class="comment">// need map-side aggregation, then write numPartitions files directly and just concatenate</span></span><br><span class="line">      <span class="comment">// them at the end. This avoids doing serialization and deserialization twice to merge</span></span><br><span class="line">      <span class="comment">// together the spilled files, which would happen with the normal code path. The downside is</span></span><br><span class="line">      <span class="comment">// having multiple files open at a time and thus more memory allocated to buffers.</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Otherwise, buffer map outputs in a deserialized form:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>选择逻辑如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>map-side aggregation</th>
<th>Partition数(RDD)</th>
<th>Serializer支持relocation</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>否</td>
<td>小于200(默认)</td>
<td>-</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>否</td>
<td>小于16777216</td>
<td>是</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody></table>
<ul>
<li><p>BypassMergeSortShuffleWriter</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shouldBypassMergeSort</span></span>(conf: <span class="type">SparkConf</span>, dep: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="comment">// We cannot bypass sorting if we need to do map-side aggregation.</span></span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bypassMergeThreshold: <span class="type">Int</span> = conf.getInt(<span class="string">&quot;spark.shuffle.sort.bypassMergeThreshold&quot;</span>, <span class="number">200</span>)</span><br><span class="line">      dep.partitioner.numPartitions &lt;= bypassMergeThreshold</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>判断是否开启combine，只能在未开启combine时使用；</li>
<li>分区数量小于 spark.shuffle.sort.bypassMergeThreshold 设置的阈值；</li>
</ol>
</li>
<li><p>UnsafeSuffleWriter</p>
<p>canUseSerializedShuffle 判断为 ture</p>
<ol>
<li> serializer 支持 relocation；</li>
<li>没有定义本地combine</li>
<li>分区数量小于16777216</li>
</ol>
</li>
<li><p>SortShuffleWriter</p>
<p>canUseSerializedShuffle 判断为false</p>
<ol>
<li> serializer 不支持 relocation；</li>
<li>定义了本地combine</li>
<li>分区数量大于 16777216</li>
</ol>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canUseSerializedShuffle</span></span>(dependency: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> shufId = dependency.shuffleId</span><br><span class="line">  <span class="keyword">val</span> numPartitions = dependency.partitioner.numPartitions</span><br><span class="line">  <span class="comment">// 判断 serializer 是否支持 relocation</span></span><br><span class="line">  <span class="keyword">if</span> (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because the serializer, &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;dependency.serializer.getClass.getName&#125;</span>, does not support object relocation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 是否是本地combine</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency.mapSideCombine) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because we need to do &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;map-side aggregation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 分区数量是否 &gt; 16777215 + 1</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPartitions &gt; <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span>) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because it has more than &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> partitions&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can use serialized shuffle for shuffle <span class="subst">$shufId</span>&quot;</span>)</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Serializer支持relocation"><a href="#Serializer支持relocation" class="headerlink" title="Serializer支持relocation"></a>Serializer支持relocation</h4><p>Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致。Serializer的这个属性会在UnsafeShuffleWriter进行排序时用到。</p>
<p>支持relocation的Serializer是KryoSerializer，Spark默认使用<strong>JavaSerializer</strong>，通过参数spark.serializer设置。</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>map端结果按照bucket顺序依次写入<strong>dataFile</strong>文件中，这么处理后，Shuffle生成的文件数显著减少了，同时还会生成indexFile文件，记录各个bucket在dataFile中的位置，用于后续reducer随机读取文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">      partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">      mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">    <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">    partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">    partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">        blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">      partitionWriters[i] =</span><br><span class="line">        blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">    <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">    <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">      partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">      partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">      writer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">    <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">        logger.error(<span class="string">&quot;Error while deleting temp file &#123;&#125;&quot;</span>, tmp.getAbsolutePath());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h3 id="UnsafeSuffleWriter"><a href="#UnsafeSuffleWriter" class="headerlink" title="UnsafeSuffleWriter"></a>UnsafeSuffleWriter</h3><p>UnsafeShuffleWriter内部使用了和<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011564172/article/details/71170205">BytesToBytesMap</a>基本相同的数据结构处理map端的输出，不过将其细化为<strong>ShuffleExternalSorter</strong>和<strong>ShuffleInMemorySorter</strong>两部分，功能如下</p>
<ul>
<li>ShuffleExternalSorter：使用MemoryBlock存储数据，每条记录包括长度信息和K-V Pair</li>
<li>ShuffleInMemorySorter：使用long数组存储每条记录对应的位置信息(page number + offset)，以及其对应的PartitionId，共8 bytes</li>
</ul>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><p>SortShuffleWriter它主要是判断在Map端是否需要本地进行combine操作。如果需要聚合，则使用PartitionedAppendOnlyMap；如果不进行combine操作，则使用PartitionedPairBuffer添加数据存放于内存中。然后无论哪一种情况都需要判断内存是否足够，如果内存不够而且又申请不到内存，则需要进行本地磁盘溢写操作，把相关的数据写入溢写到临时文件。最后把内存里的数据和磁盘溢写的临时文件的数据进行合并，如果需要则进行一次归并排序，如果没有发生溢写则是不需要归并排序，因为都在内存里。最后生成合并后的data文件和index文件。</p>
<h4 id="writer-方法"><a href="#writer-方法" class="headerlink" title="writer 方法"></a>writer 方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Write a bunch of records to this task&#x27;s output */</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 1. 判断是否有map端combine</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don&#x27;t bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s&quot;Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>创建外部排序器ExternalSorter, 只是根据是否需要本地combine与否从而决定是否传入aggregator和keyOrdering参数；</p>
</li>
<li><p>调用ExternalSorter实例的insertAll方法，插入record；如果ExternalSorter实例中用以保存record的in-memory collection的大小达到阈值，会将record按顺序溢写到磁盘文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spill the current in-memory collection to disk if needed.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param usingMap whether we&#x27;re using a map or buffer as our current in-memory collection</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;</span><br><span class="line">    estimatedSize = map.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    estimatedSize = buffer.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>构造最终的输出文件实例,其中文件名为(reduceId为0)： “shuffle_” + shuffleId + “<em>“ + mapId + “</em>“ + reduceId；</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleDataBlockId</span>(<span class="params">shuffleId: <span class="type">Int</span>, mapId: <span class="type">Int</span>, reduceId: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">BlockId</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">&quot;shuffle_&quot;</span> + shuffleId + <span class="string">&quot;_&quot;</span> + mapId + <span class="string">&quot;_&quot;</span> + reduceId + <span class="string">&quot;.data&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在输出文件名后加上uuid用于标识文件正在写入，结束后重命名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFileWith</span></span>(path: <span class="type">File</span>): <span class="type">File</span> = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">File</span>(path.getAbsolutePath + <span class="string">&quot;.&quot;</span> + <span class="type">UUID</span>.randomUUID())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用 ExternalSorter 实例的 <code>writePartitionedFile</code> 方法，将插入到该 sorter 的 record 进行排序并写入输出文件；插入到 sorter 的 record 可以是在 in-memory collection 或者在溢写文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Write all the data added into this ExternalSorter into a file in the disk store. This is</span></span><br><span class="line"><span class="comment">   * called by the SortShuffleWriter.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param blockId block ID to write to. The index file will be blockId.name + &quot;.index&quot;.</span></span><br><span class="line"><span class="comment">   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">      blockId: <span class="type">BlockId</span>,</span><br><span class="line">      outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">    <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">    <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">      <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">      <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">      <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">        <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">          it.writeNext(writer)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(partitionId) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class="line">      <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">        <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">          <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">            writer.write(elem._1, elem._2)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">          lengths(id) = segment.length</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">    lengths</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将每个 partition 的 offset 写入 index 文件方便 reduce 端 fetch 数据；</p>
</li>
<li><p>把部分信息封装到MapStatus返回；</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>


<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sevncz@xyz</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  







  






</body>
</html>
