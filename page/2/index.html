<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto+Slab:300,300italic,400,400italic,700,700italic|PT+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sevncz.xyz","root":"/","images":"/images","scheme":"Pisces","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="努力赚钱">
<meta property="og:type" content="website">
<meta property="og:title" content="Sevncz&#39;s Blog">
<meta property="og:url" content="http://sevncz.xyz/page/2/index.html">
<meta property="og:site_name" content="Sevncz&#39;s Blog">
<meta property="og:description" content="努力赚钱">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sevncz">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://sevncz.xyz/page/2/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>
<title>Sevncz's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Sevncz's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="sevncz"
      src="/images/IMG_1169.JPG">
  <p class="site-author-name" itemprop="name">sevncz</p>
  <div class="site-description" itemprop="description">努力赚钱</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">22</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">22</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Sevncz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sevncz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Redis%E5%AE%9E%E7%8E%B0%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/" class="post-title-link" itemprop="url">Redis实现分布式锁</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-22 22:38:39 / 修改时间：23:17:28" itemprop="dateCreated datePublished" datetime="2020-12-22T22:38:39+08:00">2020-12-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>和单机上的锁类似，分布式锁同样可以用一个变量来实现。客户端加锁和释放锁的操作逻辑，也和单机上的加锁和释放锁操作逻辑一致：加锁时同样需要判断锁变量的值，根据锁变量值来判断能否加锁成功；释放锁时需要把锁变量值设置为 0，表明客户端不再持有锁。</p>
<p>在分布式场景下，锁变量需要由一个共享存储系统来维护，只有这样，多个客户端才可以通过访问共享存储系统来访问锁变量。相应的，加锁和释放锁的操作就变成了读取、判断和设置共享存储系统中的锁变量值。</p>
<h2 id="分布式锁的两个要求"><a href="#分布式锁的两个要求" class="headerlink" title="分布式锁的两个要求"></a>分布式锁的两个要求</h2><ul>
<li><p>要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性；</p>
</li>
<li><p>要求二：共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。</p>
</li>
</ul>
<p>我们既可以基于单个 Redis 节点来实现，也可以使用多个 Redis 节点实现。在这两种情况下，锁的可靠性是不一样的。</p>
<h2 id="基于单个-Redis-节点实现分布式锁"><a href="#基于单个-Redis-节点实现分布式锁" class="headerlink" title="基于单个 Redis 节点实现分布式锁"></a>基于单个 Redis 节点实现分布式锁</h2><p>加锁包含了三个操作（读取锁变量、判断锁变量值以及把锁变量值设置为 1），而这三个操作在执行时需要保证原子性。那怎么保证原子性呢？</p>
<h3 id="SETNX-命令"><a href="#SETNX-命令" class="headerlink" title="SETNX 命令"></a>SETNX 命令</h3><p>它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SETNX key value</span><br></pre></td></tr></table></figure>

<p>我们就可以用 SETNX 和 DEL 命令组合来实现加锁和释放锁操作</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 加锁</span></span><br><span class="line">SETNX lock_key <span class="number">1</span></span><br><span class="line"><span class="comment">// 业务逻辑</span></span><br><span class="line">DO THINGS</span><br><span class="line"><span class="comment">// 释放锁</span></span><br><span class="line">DEL lock_key</span><br></pre></td></tr></table></figure>

<p>不过，使用 SETNX 和 DEL 命令组合实现分布锁，存在两个潜在的风险。</p>
<p>第一个风险是，加锁之后的操作发生异常，无法执行 DEL 命令释放锁，因此别的客户端也无法拿到锁，无法执行后续操作。针对这个问题，可以<strong>给锁变量加一个过期时间</strong>。</p>
<p>第二个风险是，客户端A执行了 SETNX 命令加锁后，客户端B执行了DEL命令释放锁，此时，客户端A的锁就被误释放了。针对这个问题，我们需要能<strong>区分来自不同客户端的锁操作</strong>。在使用 SETNX 命令进行加锁的方法中，我们通过把锁变量值设置为 1 或 0，表示是否加锁成功。1 和 0 只有两种状态，无法表示究竟是哪个客户端进行的锁操作。所以，我们在加锁操作时，可以让每个客户端给锁变量设置一个唯一值，这里的唯一值就可以用来标识当前操作的客户端。在释放锁操作时，客户端需要判断，当前锁变量的值是否和自己的唯一标识相等，只有在相等的情况下，才能释放锁。这样一来，就不会出现误释放锁的问题了。</p>
<h2 id="基于多个-Redis-节点实现高可靠的分布式锁"><a href="#基于多个-Redis-节点实现高可靠的分布式锁" class="headerlink" title="基于多个 Redis 节点实现高可靠的分布式锁"></a>基于多个 Redis 节点实现高可靠的分布式锁</h2><p>当我们要实现高可靠的分布式锁时，就不能依赖单个的命令操作了，我们需要安装一定的步骤和规则进行加解锁操作。否则，就可能会出现锁无法工作的情况。“一定的步骤和规则”是指啥呢？其实就是分布式锁的算法。</p>
<p>为了避免 Redis 实例故障而导致的锁无法工作的问题，Redis 的开发者 Antirez 提出了分布式锁算法 <strong>Redlock</strong>。</p>
<p>Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例一次请求加锁，如果客户端能够和半数以上的实例成功完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失。</p>
<p>Redlock 算法的实现需要有 N 个独立的 Redis 实例。以下是加锁步骤：</p>
<ol>
<li><p><strong>客户端获取当前时间；</strong></p>
</li>
<li><p><strong>客户端按顺序依次向 N 个 Redis 实例执行加锁操作；</strong></p>
<p>这里的加锁操作和在单实例上执行的加锁操作一样，使用 SET 命令，带上 NX，EX/PX 选项，以及带上客户端的唯一标识。当然，如果某个 Redis 实例发生故障了，为了保证在这种情况下，Redlock 算法能够继续运行，我们需要给加锁操作设置一个超时时间。</p>
<p>如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。</p>
</li>
<li><p><strong>一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。</strong></p>
</li>
</ol>
<p>客户端只有在满足下面的这两个条件时，才能认为是加锁成功。</p>
<ul>
<li><p><strong>条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁；</strong></p>
</li>
<li><p><strong>条件二：客户端获取锁的总耗时没有超过锁的有效时间。</strong></p>
</li>
</ul>
<p>在满足了这两个条件后，我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。</p>
<p>如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。</p>
<p>在 Redlock 算法中，释放锁的操作和在单实例上释放锁的操作一样，只要执行释放锁的 Lua 脚本就可以了。这样一来，只要 N 个 Redis 实例中的半数以上实例能正常工作，就能保证分布式锁的正常工作了。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Redis-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Redis-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Redis 主从数据同步原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>
      

      <time title="创建时间：2020-12-22 17:11:53 / 修改时间：17:12:02" itemprop="dateCreated datePublished" datetime="2020-12-22T17:11:53+08:00">2020-12-22</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/" class="post-title-link" itemprop="url">Spark ShuffleManager的运行原理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-22 15:01:22" itemprop="dateCreated datePublished" datetime="2020-12-22T15:01:22+08:00">2020-12-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-01-19 12:14:26" itemprop="dateModified" datetime="2021-01-19T12:14:26+08:00">2021-01-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h2><p>有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。</p>
<h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p>先看下SortShuffleManager的官方说明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * In sort-based shuffle, incoming records are sorted according to their target partition ids, then</span></span><br><span class="line"><span class="comment"> * written to a single map output file. Reducers fetch contiguous regions of this file in order to</span></span><br><span class="line"><span class="comment"> * read their portion of the map output. In cases where the map output data is too large to fit in</span></span><br><span class="line"><span class="comment"> * memory, sorted subsets of the output can be spilled to disk and those on-disk files are merged</span></span><br><span class="line"><span class="comment"> * to produce the final output file.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> # 有两种不同的方式输出文件</span></span><br><span class="line"><span class="comment"> * Sort-based shuffle has two different write paths for producing its map output files:</span></span><br><span class="line"><span class="comment"> # 序列化排序，当一下三个条件都成立时使用</span></span><br><span class="line"><span class="comment"> *  - Serialized sorting: used when all three of the following conditions hold:</span></span><br><span class="line"><span class="comment"> # shuffle 的依赖没有指定的聚合或输出排序</span></span><br><span class="line"><span class="comment"> *    1. The shuffle dependency specifies no aggregation or output ordering.</span></span><br><span class="line"><span class="comment"> # shuffle 的序列化支持序列化值的重定位</span></span><br><span class="line"><span class="comment"> *    2. The shuffle serializer supports relocation of serialized values (this is currently supported by KryoSerializer and Spark SQL&#x27;s custom serializers).</span></span><br><span class="line"><span class="comment"> # shuffle 产生的输出分区少于16777216</span></span><br><span class="line"><span class="comment"> *    3. The shuffle produces fewer than 16777216 output partitions.</span></span><br><span class="line"><span class="comment"> *  - Deserialized sorting: used to handle all other cases.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> * Serialized sorting mode</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 优化手段</span></span><br><span class="line"><span class="comment"> * In the serialized sorting mode, incoming records are serialized as soon as they are passed to the</span></span><br><span class="line"><span class="comment"> * shuffle writer and are buffered in a serialized form during sorting. This write path implements</span></span><br><span class="line"><span class="comment"> * several optimizations:</span></span><br><span class="line"><span class="comment"> # 排序在二进制数据上而不是Java对象</span></span><br><span class="line"><span class="comment"> *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory</span></span><br><span class="line"><span class="comment"> *    consumption and GC overheads. This optimization requires the record serializer to have certain</span></span><br><span class="line"><span class="comment"> *    properties to allow serialized records to be re-ordered without requiring deserialization.</span></span><br><span class="line"><span class="comment"> *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 使用一个专门的缓存效率高的分拣器([[ShuffleExternalSorter]])，对压缩记录指针和分区ID的数组进行分拣。通过在排序数组中每条记录只使用8个字节的空间，这可以将更多的数组放入缓存。</span></span><br><span class="line"><span class="comment"> *  - It uses a specialized cache-efficient sorter ([[ShuffleExternalSorter]]) that sorts</span></span><br><span class="line"><span class="comment"> *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per</span></span><br><span class="line"><span class="comment"> *    record in the sorting array, this fits more of the array into cache.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 溢出合并程序对属于同一分区的序列化记录块进行操作，在合并过程中不需要反序列化记录。</span></span><br><span class="line"><span class="comment"> *  - The spill merging procedure operates on blocks of serialized records that belong to the same</span></span><br><span class="line"><span class="comment"> *    partition and does not need to deserialize records during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> #  当spill压缩编解码器支持压缩数据的连接时，spill合并只是将序列化和压缩的spill分区连接起来，产生最终的输出分区。 这样就可以使用高效的数据复制方法，如NIO的 &quot;transferTo&quot;，并避免了在合并过程中分配解压或复制缓冲区的需要。</span></span><br><span class="line"><span class="comment"> *  - When the spill compression codec supports concatenation of compressed data, the spill merge</span></span><br><span class="line"><span class="comment"> *    simply concatenates the serialized and compressed spill partitions to produce the final output</span></span><br><span class="line"><span class="comment"> *    partition.  This allows efficient data copying methods, like NIO&#x27;s `transferTo`, to be used</span></span><br><span class="line"><span class="comment"> *    and avoids the need to allocate decompression or copying buffers during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For more details on these optimizations, see SPARK-7081.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>

<p>主要方法</p>
<ul>
<li>registerShuffle</li>
<li>getReader</li>
<li>getWriter</li>
<li>unregisterShuffle</li>
</ul>
<h4 id="三种-writer-说明及选择时机"><a href="#三种-writer-说明及选择时机" class="headerlink" title="三种 writer 说明及选择时机"></a>三种 writer 说明及选择时机</h4><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>和Hash Shuffle实现基本相同，区别在于map task输出会汇总为一个文件</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>tungsten-sort，ShuffleExternalSorter使用Java Unsafe直接操作内存，避免Java对象多余的开销和GC 延迟，效率高</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>Sort Shuffle，和Hash Shuffle的主要不同在于，map端支持Partition级别的sort，map task输出会汇总为一个文件</td>
</tr>
</tbody></table>
<p>Spark根据运行时信息选择三种ShuffleWriter实现中的一种，对应的源码为<strong>SortShuffleManager</strong>中的<strong>registerShuffle</strong>方法，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Obtains a [[ShuffleHandle]] to pass to tasks.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      numMaps: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="comment">// If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#x27;t</span></span><br><span class="line">      <span class="comment">// need map-side aggregation, then write numPartitions files directly and just concatenate</span></span><br><span class="line">      <span class="comment">// them at the end. This avoids doing serialization and deserialization twice to merge</span></span><br><span class="line">      <span class="comment">// together the spilled files, which would happen with the normal code path. The downside is</span></span><br><span class="line">      <span class="comment">// having multiple files open at a time and thus more memory allocated to buffers.</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Otherwise, buffer map outputs in a deserialized form:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>选择逻辑如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>map-side aggregation</th>
<th>Partition数(RDD)</th>
<th>Serializer支持relocation</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>否</td>
<td>小于200(默认)</td>
<td>-</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>否</td>
<td>小于16777216</td>
<td>是</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody></table>
<ul>
<li><p>BypassMergeSortShuffleWriter</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shouldBypassMergeSort</span></span>(conf: <span class="type">SparkConf</span>, dep: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="comment">// We cannot bypass sorting if we need to do map-side aggregation.</span></span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bypassMergeThreshold: <span class="type">Int</span> = conf.getInt(<span class="string">&quot;spark.shuffle.sort.bypassMergeThreshold&quot;</span>, <span class="number">200</span>)</span><br><span class="line">      dep.partitioner.numPartitions &lt;= bypassMergeThreshold</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>判断是否开启combine，只能在未开启combine时使用；</li>
<li>分区数量小于 spark.shuffle.sort.bypassMergeThreshold 设置的阈值；</li>
</ol>
</li>
<li><p>UnsafeSuffleWriter</p>
<p>canUseSerializedShuffle 判断为 ture</p>
<ol>
<li> serializer 支持 relocation；</li>
<li>没有定义本地combine</li>
<li>分区数量小于16777216</li>
</ol>
</li>
<li><p>SortShuffleWriter</p>
<p>canUseSerializedShuffle 判断为false</p>
<ol>
<li> serializer 不支持 relocation；</li>
<li>定义了本地combine</li>
<li>分区数量大于 16777216</li>
</ol>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canUseSerializedShuffle</span></span>(dependency: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> shufId = dependency.shuffleId</span><br><span class="line">  <span class="keyword">val</span> numPartitions = dependency.partitioner.numPartitions</span><br><span class="line">  <span class="comment">// 判断 serializer 是否支持 relocation</span></span><br><span class="line">  <span class="keyword">if</span> (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because the serializer, &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;dependency.serializer.getClass.getName&#125;</span>, does not support object relocation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 是否是本地combine</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency.mapSideCombine) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because we need to do &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;map-side aggregation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 分区数量是否 &gt; 16777215 + 1</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPartitions &gt; <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span>) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because it has more than &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> partitions&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can use serialized shuffle for shuffle <span class="subst">$shufId</span>&quot;</span>)</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Serializer支持relocation"><a href="#Serializer支持relocation" class="headerlink" title="Serializer支持relocation"></a>Serializer支持relocation</h4><p>Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致。Serializer的这个属性会在UnsafeShuffleWriter进行排序时用到。</p>
<p>支持relocation的Serializer是KryoSerializer，Spark默认使用<strong>JavaSerializer</strong>，通过参数spark.serializer设置。</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>map端结果按照bucket顺序依次写入<strong>dataFile</strong>文件中，这么处理后，Shuffle生成的文件数显著减少了，同时还会生成indexFile文件，记录各个bucket在dataFile中的位置，用于后续reducer随机读取文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">      partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">      mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">    <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">    partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">    partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">        blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">      partitionWriters[i] =</span><br><span class="line">        blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">    <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">    <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">      partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">      partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">      writer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">    <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">        logger.error(<span class="string">&quot;Error while deleting temp file &#123;&#125;&quot;</span>, tmp.getAbsolutePath());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h3 id="UnsafeSuffleWriter"><a href="#UnsafeSuffleWriter" class="headerlink" title="UnsafeSuffleWriter"></a>UnsafeSuffleWriter</h3><p>UnsafeShuffleWriter内部使用了和<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011564172/article/details/71170205">BytesToBytesMap</a>基本相同的数据结构处理map端的输出，不过将其细化为<strong>ShuffleExternalSorter</strong>和<strong>ShuffleInMemorySorter</strong>两部分，功能如下</p>
<ul>
<li>ShuffleExternalSorter：使用MemoryBlock存储数据，每条记录包括长度信息和K-V Pair</li>
<li>ShuffleInMemorySorter：使用long数组存储每条记录对应的位置信息(page number + offset)，以及其对应的PartitionId，共8 bytes</li>
</ul>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><p>SortShuffleWriter它主要是判断在Map端是否需要本地进行combine操作。如果需要聚合，则使用PartitionedAppendOnlyMap；如果不进行combine操作，则使用PartitionedPairBuffer添加数据存放于内存中。然后无论哪一种情况都需要判断内存是否足够，如果内存不够而且又申请不到内存，则需要进行本地磁盘溢写操作，把相关的数据写入溢写到临时文件。最后把内存里的数据和磁盘溢写的临时文件的数据进行合并，如果需要则进行一次归并排序，如果没有发生溢写则是不需要归并排序，因为都在内存里。最后生成合并后的data文件和index文件。</p>
<h4 id="writer-方法"><a href="#writer-方法" class="headerlink" title="writer 方法"></a>writer 方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Write a bunch of records to this task&#x27;s output */</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 1. 判断是否有map端combine</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don&#x27;t bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s&quot;Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>创建外部排序器ExternalSorter, 只是根据是否需要本地combine与否从而决定是否传入aggregator和keyOrdering参数；</p>
</li>
<li><p>调用ExternalSorter实例的insertAll方法，插入record；如果ExternalSorter实例中用以保存record的in-memory collection的大小达到阈值，会将record按顺序溢写到磁盘文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spill the current in-memory collection to disk if needed.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param usingMap whether we&#x27;re using a map or buffer as our current in-memory collection</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;</span><br><span class="line">    estimatedSize = map.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    estimatedSize = buffer.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>构造最终的输出文件实例,其中文件名为(reduceId为0)： “shuffle_” + shuffleId + “<em>“ + mapId + “</em>“ + reduceId；</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleDataBlockId</span>(<span class="params">shuffleId: <span class="type">Int</span>, mapId: <span class="type">Int</span>, reduceId: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">BlockId</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">&quot;shuffle_&quot;</span> + shuffleId + <span class="string">&quot;_&quot;</span> + mapId + <span class="string">&quot;_&quot;</span> + reduceId + <span class="string">&quot;.data&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在输出文件名后加上uuid用于标识文件正在写入，结束后重命名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFileWith</span></span>(path: <span class="type">File</span>): <span class="type">File</span> = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">File</span>(path.getAbsolutePath + <span class="string">&quot;.&quot;</span> + <span class="type">UUID</span>.randomUUID())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用 ExternalSorter 实例的 <code>writePartitionedFile</code> 方法，将插入到该 sorter 的 record 进行排序并写入输出文件；插入到 sorter 的 record 可以是在 in-memory collection 或者在溢写文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Write all the data added into this ExternalSorter into a file in the disk store. This is</span></span><br><span class="line"><span class="comment">   * called by the SortShuffleWriter.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param blockId block ID to write to. The index file will be blockId.name + &quot;.index&quot;.</span></span><br><span class="line"><span class="comment">   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">      blockId: <span class="type">BlockId</span>,</span><br><span class="line">      outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">    <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">    <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">      <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">      <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">      <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">        <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">          it.writeNext(writer)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(partitionId) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class="line">      <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">        <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">          <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">            writer.write(elem._1, elem._2)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">          lengths(id) = segment.length</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">    lengths</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将每个 partition 的 offset 写入 index 文件方便 reduce 端 fetch 数据；</p>
</li>
<li><p>把部分信息封装到MapStatus返回；</p>
</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/%E6%97%81%E8%B7%AF%E7%BC%93%E5%AD%98%EF%BC%9ARedis%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/22/%E6%97%81%E8%B7%AF%E7%BC%93%E5%AD%98%EF%BC%9ARedis%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8/" class="post-title-link" itemprop="url">旁路缓存：Redis缓存应用</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-22 09:38:37" itemprop="dateCreated datePublished" datetime="2020-12-22T09:38:37+08:00">2020-12-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-02-22 17:17:22" itemprop="dateModified" datetime="2021-02-22T17:17:22+08:00">2021-02-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="缓存的特征"><a href="#缓存的特征" class="headerlink" title="缓存的特征"></a>缓存的特征</h2><ol>
<li>在一个层次化的系统中，缓存一定是一个快速子系统，数据存在缓存中时，能避免每次从慢速子系统中存取数据；</li>
<li>缓存系统的容量大小总是小于后端慢速系统，我们不可能把所有数据都放在缓存系统中；</li>
</ol>
<h2 id="Redis-缓存处理请求的两种情况"><a href="#Redis-缓存处理请求的两种情况" class="headerlink" title="Redis 缓存处理请求的两种情况"></a>Redis 缓存处理请求的两种情况</h2><ul>
<li>缓存命中：Redis 中有相应的数据，就直接读取 Redis，性能非常快。</li>
<li>缓存缺失：Redis 中没有保存相应的数据，就从后端数据库中读取数据，性能就会变慢。而且，一旦发生缓存缺失，为了让后续请求能从缓存中读取到数据，我们需要把缺失的数据写入Redis，这个过程叫做<strong>缓存更新</strong>，缓存更新操作会设计到保证缓存和数据库之间的数据一致性问题。</li>
</ul>
<h2 id="Redis-作为旁路缓存的使用操作"><a href="#Redis-作为旁路缓存的使用操作" class="headerlink" title="Redis 作为旁路缓存的使用操作"></a>Redis 作为旁路缓存的使用操作</h2><p>旁路缓存：Redis 不主动访问数据库加载数据到自己的内存中，而是在应用程序读取Redis缓存，如果不存在，就去查后端数据库，再将查询到的数据加载到Redis。</p>
<p>在使用旁路缓存时，需要在应用程序中增加操作代码，增加了使用Redis缓存的额外工作量，但是，也正是因为Redis是旁路缓存，是一个独立的系统，我们可以单独对Redis缓存进行扩容或性能优化。而且，只要保持操作接口不变，我们在应用程序中增加的代码就不用再修改了。</p>
<p>Redis 缓存的两种类型：</p>
<ul>
<li>只读缓存：能加速读请求</li>
<li>读写缓存：同时加速读写请求，读写缓存又有两种数据写回策略，可以让我们根据业务需求，在保证性能和保证数据可靠性之间进行选择。</li>
</ul>
<h2 id="缓存类型"><a href="#缓存类型" class="headerlink" title="缓存类型"></a>缓存类型</h2><h3 id="只读缓存"><a href="#只读缓存" class="headerlink" title="只读缓存"></a>只读缓存</h3><p><strong>读请求</strong></p>
<p>先调用 Redis 的 Get 接口，查询数据是否存在，如果发生缓存缺失，应用会把这些数据从数据库读出来，并写到缓存中。</p>
<p><strong>写请求</strong></p>
<p>会直接发往后端的数据库，在数据库中增删改。对于删改的数据来说，应用需要把这些缓存的数据删除，Redis 中就没有这些数据了。</p>
<p>这里最新的数据在数据库中。</p>
<h3 id="读写缓存"><a href="#读写缓存" class="headerlink" title="读写缓存"></a>读写缓存</h3><p>对于读写缓存来说，除了读请求会发送到缓存进行处理，写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。由于Redis的高性能访问特性，数据的增删改操作可以在缓存中快速完成，处理结果也会快速返回给业务应用，提升业务响应速度。</p>
<p>这里，最新的数据是在Reids中，一旦出现掉电或宕机，内存中的数据就会丢失。</p>
<p>所以，根据业务应用对数据可靠性和缓存性能的不同要求，我们会有同步直写和异步写回两种策略。其中，同步直写策略优先保证数据可靠性，异步写回策略优先提供快速响应。</p>
<p><strong>同步直写</strong></p>
<p>写请求发给缓存的同时，也会发给后端数据库进行处理，等到缓存和数据库都处理完数据，才给客户端返回。</p>
<p><strong>异步写回</strong></p>
<p>写请求先在缓存处理，等这些增改的数据要被从缓存中淘汰出来时，缓存将他们写回后端数据库。</p>
<h2 id="缓存容量设置"><a href="#缓存容量设置" class="headerlink" title="缓存容量设置"></a>缓存容量设置</h2><p>结合应用数据实际访问特征和成本开销来综合考虑。</p>
<p>一般来说，缓存容量设置为总数据量的15%到30%，兼顾访问性能和内存空间开销。</p>
<p>Redis可以通过以下命令设定缓存的大小：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONFIG SET maxmemory 4gb</span><br></pre></td></tr></table></figure>

<p>缓存写满是不可避免的，所以如何淘汰数据以及如何处理那些被淘汰的数据需要了解。</p>
<h2 id="Redis-缓存的淘汰策略"><a href="#Redis-缓存的淘汰策略" class="headerlink" title="Redis 缓存的淘汰策略"></a>Redis 缓存的淘汰策略</h2><p>一共有8种策略，4.0以前6种，之后增加了2种，其中，只有一种noeviction不淘汰数据的策略，七种淘汰数据的策略。</p>
<p>七种淘汰策略可以根据淘汰候选数据集的范围把它们分为两类：</p>
<ul>
<li>在设置了过期时间的数据中进行淘汰，包括：volatile-random、volatile-ttl、volatile-lru、volatile-lfu（4.0后增加） 这四种。</li>
<li>在所有数据范围内进行淘汰，包括：allkeys-lru、allkeys-random、allkeys-lfu（4.0后增加）三种。</li>
</ul>
<p>默认情况下，Reids 在使用的内存空间操作maxmemory值时，并不会淘汰数据，也就是设定的 noeviction 策略。对应到Redis缓存，也就是指，一旦缓存被写满，写请求再来时，Redis 不再提供服务，而是直接返回错误。</p>
<p>设置过期时间的数据，当过期时间快到期，或者达到了Maxmemory阈值，Redis会按照以下策略进行淘汰。</p>
<ul>
<li>volatile-ttl：根据过期时间的先后进行删除，越早过期的越先被删除。</li>
<li>volatile-random：设置了过期时间的键值对中进行随机删除。</li>
<li>volatile-lru：以LRU算法筛选</li>
<li>volatile-lfu：以LFU算法筛选</li>
</ul>
<p>和以上策略类似，对于所有数据也有以下3类淘汰策略:</p>
<ul>
<li>allkeys-random</li>
<li>allkeys-lru</li>
<li>allkeys-lfu</li>
</ul>
<h3 id="LRU算法"><a href="#LRU算法" class="headerlink" title="LRU算法"></a>LRU算法</h3><p>全称 Least Recently Used，按最近最少使用的原则筛选数据。最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。</p>
<p>LRU 会把所有的数据组织成一个链表，链表的头和尾分别表示MRU端和LRU端，分别代表最近最常使用的数据和最近最不常用的数据。删除数据时从LRU端开始。</p>
<p>每有一个数据被访问，该数据就会放入链表头，链表中其他数据则相应向后移一位。</p>
<p>LRU在实际实现时，会创建一个额外的链表来管理所有缓存数据，带来了额外的空间开销。当有大量数据被访问时，就会带来很多链表移动操作，会很耗时，进而降低性能。</p>
<p>在Redis中，LRU 算法被进行了优化，以减轻数据淘汰对缓存性能的影响。具体来说，Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把他们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。（局部数据淘汰机制，所以 Redis 中可能存在长时间未删除的过期数据）</p>
<p>可以配置参数 maxmemory-samples 来设置 N 的个数。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CONFIG SET maxmemory-samples 100</span><br></pre></td></tr></table></figure>

<p>当需要再次淘汰数据时，Redis 需要挑选数据进入第一次淘汰时创建的候选集合。这里的挑选标准是：<strong>能进入候选集合的数据的 lru 字段值必须小于候选集合中最小的 lru 值。</strong> 当有新数据进入候选数据集后，如果候选数据集中的数据个数达到了 maxmemory-samples，Redis 就把候选数据集中 lru 字段最小的数据淘汰出去。</p>
<p>这样 Redis 缓存不用为所有数据维护一个大链表，也不用在每次数据访问时都移动链表项，提升了缓存的性能。</p>
<h3 id="缓存淘汰策略使用的建议"><a href="#缓存淘汰策略使用的建议" class="headerlink" title="缓存淘汰策略使用的建议"></a>缓存淘汰策略使用的建议</h3><ul>
<li>优先使用 allkeys-lru 策略</li>
<li>业务应用的数据访问频率相差不大，没有明显的冷热数据区分，建议使用 allkeys-random 策略</li>
<li>如果业务中有置顶需求，比如置顶新闻、置顶视频，那么可以使用 volatile-lru 策略，同时不给这些置顶数据设置过期时间。</li>
</ul>
<h3 id="如何处理被淘汰的数据"><a href="#如何处理被淘汰的数据" class="headerlink" title="如何处理被淘汰的数据"></a>如何处理被淘汰的数据</h3><p>一般来说，一个数据被淘汰策略选定之后，如果这个数据是干净数据，那么直接删除；如果这个数据是脏数据，那么我们需要把它写回数据库。</p>
<p>如何判断数据是干净还是脏？</p>
<p>干净数据和脏数据的区别就在于，和最初从后端数据库里读取时的值相比，有没有被修改过。修改过的值需要写回数据库，保证数据的一致性。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/16/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/12/16/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/" class="post-title-link" itemprop="url">Redis持久化机制</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-16 10:16:48" itemprop="dateCreated datePublished" datetime="2020-12-16T10:16:48+08:00">2020-12-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2020-12-22 22:39:55" itemprop="dateModified" datetime="2020-12-22T22:39:55+08:00">2020-12-22</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Redis/" itemprop="url" rel="index"><span itemprop="name">Redis</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>Redis 的数据存在内存中，这样响应速度会非常快，但是不能忽视的问题是，一旦 Redis 宕机，数据就会丢失，如果从后端数据库恢复这些数据，会存在两个问题：</p>
<ol>
<li>需要频繁访问数据库，会给数据库带来巨大的压力；</li>
<li>这些数据是从慢速数据库中读取出来的，性能肯定比不上从 Redis 中读取，导致使用这些数据的应用程序响应变慢；</li>
</ol>
<p>所以，Redis 提供了两个持久化机制，AOF（Append Only File）日志 和 RDB（Redis DataBase）快照。</p>
<h2 id="AOF日志实现"><a href="#AOF日志实现" class="headerlink" title="AOF日志实现"></a>AOF日志实现</h2><p>一般数据库的日志是写前日志（WFL，Write Ahead Log），也就是在数据写入前，先把修改的数据记录到日志文件中，以便发生故障时恢复。不过，AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志。</p>
<h3 id="AOF日志内容"><a href="#AOF日志内容" class="headerlink" title="AOF日志内容"></a>AOF日志内容</h3><p>传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。</p>
<h3 id="AOF写入优化机制"><a href="#AOF写入优化机制" class="headerlink" title="AOF写入优化机制"></a>AOF写入优化机制</h3><p>为了避免额外的检查开销，Redis 在向 AOF 写入日志的时候，不会先去对命令进行语法检查，所以，如果先记录日志再执行命令的话，可能在日志文件中记录了错误的命令，Redis 在恢复数据的时候，就可能会出错。</p>
<p>而后写日志机制，就是先让系统执行命令，成功之后再记录日志，这样就可以避免出现记录错误命令的情况。</p>
<p>此外，AOF 后写机制还有一个好处：它是在命令执行后才记录日志，所以不会阻塞当前的写操作。</p>
<p>当然 AOF 也有两个潜在的风险：</p>
<ol>
<li>数据丢失，在还未来得及写入日志就发生宕机时；</li>
<li>阻塞其他操作，虽然避免了对当前操作的阻塞，但可能对下一个操作带来阻塞风险，这是因为 AOF 日志也是在主线程中执行的，如果在写把日志写入磁盘时，系统磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行；</li>
</ol>
<h3 id="三种写磁盘的方式"><a href="#三种写磁盘的方式" class="headerlink" title="三种写磁盘的方式"></a>三种写磁盘的方式</h3><p>AOF 提供了三种写回磁盘的选择，也就是 AOF 配置项 appendfsync 的三个可选值。</p>
<ul>
<li>Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；</li>
<li>Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；</li>
<li>No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。</li>
</ul>
<p>针对避免主线程阻塞和减少数据丢失问题，这三种写回策略都无法做到两全其美。</p>
<ul>
<li>“同步写回”可以做到基本不丢数据，但是它在每一个写命令后都有一个慢速的落盘操作，不可避免地会影响主线程性能；</li>
<li>虽然“操作系统控制的写回”在写完缓冲区后，就可以继续执行后续的命令，但是落盘的时机已经不在 Redis 手中了，只要 AOF 记录没有写回磁盘，一旦宕机对应的数据就丢失了；</li>
<li>“每秒写回”采用一秒写回一次的频率，避免了“同步写回”的性能开销，虽然减少了对系统性能的影响，但是如果发生宕机，上一秒内未落盘的命令操作仍然会丢失。所以，这只能算是，在避免影响主线程性能和避免数据丢失两者间取了个折中。</li>
</ul>
<p>三种写回时机的优缺点如下：</p>
<img src="/2020/12/16/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/AOF%E5%86%99%E5%9B%9E%E6%9C%BA%E5%88%B6%E4%BC%98%E7%BC%BA%E7%82%B9.jpg" class="">

<p>到这里，我们就可以根据系统对高性能和高可靠性的要求，来选择使用哪种写回策略了。总结一下就是：想要获得高性能，就选择 No 策略；如果想要得到高可靠性保证，就选择 Always 策略；如果允许数据有一点丢失，又希望性能别受太大影响的话，那么就选择 Everysec 策略。</p>
<h3 id="AOF-的性能问题"><a href="#AOF-的性能问题" class="headerlink" title="AOF 的性能问题"></a>AOF 的性能问题</h3><p>AOF 是以文件的形式在记录接收到的所有写命令。随着接收的写命令越来越多，AOF 文件会越来越大。</p>
<p>AOF 文件过大会会导致性能问题，主要在于以下3个方面：</p>
<ol>
<li><p>操作系统对文件大小有限制，不能无限增大。 </p>
</li>
<li><p>过大的文件，追加记录效率低。 </p>
</li>
<li><p>Redis异常关闭重启之后，AOF 记录的命令要一个一个被重新执行，文件过大导致执行时间长，影响Redis可用性。</p>
</li>
</ol>
<h3 id="AOF-重写机制"><a href="#AOF-重写机制" class="headerlink" title="AOF 重写机制"></a>AOF 重写机制</h3><p>为解决 AOF 文件过大导致的性能为题，Redis 提供了 AOF 重写机制。</p>
<p>AOF重写机制指的是，对过大的AOF文件进行重写，以此来压缩AOF文件的大小。</p>
<p>具体的实现是：检查当前键值数据库中的键值对，记录键值对的最终状态，从而实现对 某个键值对 重复操作后产生的多条操作记录压缩成一条 的效果。进而实现压缩AOF文件的大小。</p>
<p>AOF 重写过程是由后台子进程 <strong>bgrewriteaof</strong> 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。</p>
<p>AOF 重写日志的过程总结为两部分：</p>
<ol>
<li>一次拷贝</li>
<li>两处日志</li>
</ol>
<p>“一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。</p>
<p>“两处日志”又是什么呢？因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。</p>
<img src="/2020/12/16/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/AOF%E9%9D%9E%E9%98%BB%E5%A1%9E%E7%9A%84%E9%87%8D%E5%86%99%E8%BF%87%E7%A8%8B.jpg" class="">

<p>总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。</p>
<h2 id="RDB-文件实现"><a href="#RDB-文件实现" class="headerlink" title="RDB 文件实现"></a>RDB 文件实现</h2><p>AOF 的好处：只需要记录日志，坏处：宕机之后恢复慢。</p>
<p>RDB把某一时刻的状态以文件的形式写到磁盘上，即使宕机，RDB也不会丢失。</p>
<p>和 AOF 相比，RDB 记录的是某一时刻的数据，并不是操作，所以，在做数据恢复时，我们可以直接把 RDB 文件读入内存，很快地完成恢复。</p>
<h3 id="如何避免快照时阻塞"><a href="#如何避免快照时阻塞" class="headerlink" title="如何避免快照时阻塞"></a>如何避免快照时阻塞</h3><p>Redis 提供了两个命令来生成RDB文件，分别是save和bgsave。</p>
<ul>
<li><p>save：在主线程中执行，会导致阻塞；</p>
</li>
<li><p>bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。</p>
</li>
</ul>
<p>bgsave 可以避免阻塞，但避免阻塞和正常处理写操作并不是一回事。此时，主线程的确没有阻塞，可以正常接收请求，但是，为了保证快照完整性，它只能处理读操作，因为不能修改正在执行快照的数据。</p>
<p>为了快照而暂停写操作，肯定是不能接受的。所以这个时候，Redis 就会借助操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。</p>
<p>简单来说，bgsave 子进程是由主线程 fork 生成的，可以共享主线程的所有内存数据。bgsave 子进程运行后，开始读取主线程的内存数据，并把它们写入 RDB 文件。</p>
<p>此时，如果主线程对这些数据也都是读操作（例如图中的键值对 A），那么，主线程和 bgsave 子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对 C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave 子进程会把这个副本数据写入 RDB 文件，而在这个过程中，主线程仍然可以直接修改原来的数据。</p>
<img src="/2020/12/16/Redis%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/%E5%86%99%E6%97%B6%E5%A4%8D%E5%88%B6%E6%9C%BA%E5%88%B6%E4%BF%9D%E8%AF%81%E5%BF%AB%E7%85%A7%E6%9C%9F%E9%97%B4%E6%95%B0%E6%8D%AE%E5%8F%AF%E4%BF%AE%E6%94%B9.jpg" class="">

<p>这既保证了快照的完整性，也允许主线程同时对数据进行修改，避免了对正常业务的影响。</p>
<h3 id="RDB的执行频率"><a href="#RDB的执行频率" class="headerlink" title="RDB的执行频率"></a>RDB的执行频率</h3><p>虽然 bgsave 执行时不阻塞主线程，但是，如果频繁地执行全量快照，也会带来两方面的开销。</p>
<ol>
<li><p>会给磁盘造成很大的压力 </p>
</li>
<li><p>fork子进程会阻塞主线程，主线程的内存越大，fork子线程时，阻塞的时间就会越长（fork操作执行时，内核需要给子进程拷贝主线程的页表。如果主线程的内存大，页表也相应大，拷贝页表耗时长，会阻塞主线程。）</p>
</li>
</ol>
<p>此时，我们可以做增量快照，所谓增量快照，就是指，做了一次全量快照后，后续的快照只对修改的数据进行快照记录，这样可以避免每次全量快照的开销。</p>
<p>虽然跟 AOF 相比，快照的恢复速度快，但是，快照的频率不好把握，如果频率太低，两次快照间一旦宕机，就可能有比较多的数据丢失。如果频率太高，又会产生额外开销，那么，还有什么方法既能利用 RDB 的快速恢复，又能以较小的开销做到尽量少丢数据呢？</p>
<p>Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用 AOF 日志记录这期间的所有命令操作。</p>
<p>这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。</p>
<p>这个方法既能享受到 RDB 文件快速恢复的好处，又能享受到 AOF 只记录操作命令的简单优势。</p>
<h3 id="混合使用-AOF-日志和内存快照"><a href="#混合使用-AOF-日志和内存快照" class="headerlink" title="混合使用 AOF 日志和内存快照"></a>混合使用 AOF 日志和内存快照</h3><p>在Redis4.0以前，Redis AOF的重写机制是指令整合，但是在 Redis4.0 以后，Redis 的 AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头，将增量的以指令的方式Append到AOF，这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分就是压缩格式不再是 AOF 格式，可读性较差。Redis服务在读取AOF文件的怎么判断是否AOF文件中是否包含RDB，它会查看是否以 REDIS 开头；人为的看的话，也可以看到以REDIS开头，RDB的文件也打开也是乱码。</p>
<p>可以通过aof-use-rdb-preamble 配置去设置改功能。</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># When rewriting the AOF file, Redis is able to use an RDB preamble in the</span></span><br><span class="line"><span class="comment"># AOF file for faster rewrites and recoveries. When this option is turned</span></span><br><span class="line"><span class="comment"># on the rewritten AOF file is composed of two different stanzas:</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># RDB file</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># When loading Redis recognizes that the AOF file starts with the &quot;REDIS&quot;</span></span><br><span class="line"><span class="comment"># string and loads the prefixed RDB file, and continues loading the AOF</span></span><br><span class="line"><span class="comment"># tail.</span></span><br><span class="line"><span class="meta">aof-use-rdb-preamble</span> <span class="string">yes</span></span><br></pre></td></tr></table></figure>

<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>关于 AOF 和 RDB 的选择问题，有三点建议：</p>
<ul>
<li>数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择；</li>
<li>如果允许分钟级别的数据丢失，可以只使用 RDB；</li>
<li>如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。</li>
</ul>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>我们使用一个 2 核 CPU、4GB 内存、500GB 磁盘的云主机运行 Redis，Redis 数据库的数据量大小差不多是 2GB，我们使用了 RDB 做持久化保证。当时 Redis 的运行负载以修改操作为主，写读比例差不多在 8:2 左右，也就是说，如果有 100 个请求，80 个请求执行的是修改操作。在这个场景下，用 RDB 做持久化有什么风险吗？</p>
<p>风险主要在于 CPU资源 和 内存资源 这2方面：</p>
<p>a、内存资源风险：Redis fork子进程做RDB持久化，由于写的比例为80%，那么在持久化过程中，“写实复制”会重新分配整个实例80%的内存副本，大约需要重新分配1.6GB内存空间，这样整个系统的内存使用接近饱和，如果此时父进程又有大量新key写入，很快机器内存就会被吃光，如果机器开启了Swap机制，那么Redis会有一部分数据被换到磁盘上，当Redis访问这部分在磁盘上的数据时，性能会急剧下降，已经达不到高性能的标准（可以理解为武功被废）。如果机器没有开启Swap，会直接触发OOM，父子进程会面临被系统kill掉的风险。</p>
<p>b、CPU资源风险：虽然子进程在做RDB持久化，但生成RDB快照过程会消耗大量的CPU资源，虽然Redis处理处理请求是单线程的，但Redis Server还有其他线程在后台工作，例如AOF每秒刷盘、异步关闭文件描述符这些操作。由于机器只有2核CPU，这也就意味着父进程占用了超过一半的CPU资源，此时子进程做RDB持久化，可能会产生CPU竞争，导致的结果就是父进程处理请求延迟增大，子进程生成RDB快照的时间也会变长，整个Redis Server性能下降。</p>
<p>c、另外，可以再延伸一下，老师的问题没有提到Redis进程是否绑定了CPU，如果绑定了CPU，那么子进程会继承父进程的CPU亲和性属性，子进程必然会与父进程争夺同一个CPU资源，整个Redis Server的性能必然会受到影响！所以如果Redis需要开启定时RDB和AOF重写，进程一定不要绑定CPU。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/5/">5</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>


<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sevncz@xyz</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>






</body>
</html>
