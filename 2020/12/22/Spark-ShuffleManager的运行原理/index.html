<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto+Slab:300,300italic,400,400italic,700,700italic|PT+Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sevncz.xyz","root":"/","images":"/images","scheme":"Pisces","version":"8.1.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}};
  </script>
<meta name="description" content="什么是Shuffle有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。 SortShuffleManager运行原理先看下SortShuffleManager的官方说明 123456789101112131415161718192021222324252627282930313233343536373839404">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark ShuffleManager的运行原理">
<meta property="og:url" content="http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/index.html">
<meta property="og:site_name" content="Sevncz&#39;s Blog">
<meta property="og:description" content="什么是Shuffle有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。 SortShuffleManager运行原理先看下SortShuffleManager的官方说明 123456789101112131415161718192021222324252627282930313233343536373839404">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-12-22T07:01:22.000Z">
<meta property="article:modified_time" content="2021-01-19T04:14:26.797Z">
<meta property="article:author" content="sevncz">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="spark">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<title>Spark ShuffleManager的运行原理 | Sevncz's Blog</title>
  



  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Sevncz's Blog</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFShuffle"><span class="nav-number">1.</span> <span class="nav-text">什么是Shuffle</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SortShuffleManager%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">SortShuffleManager运行原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%89%E7%A7%8D-writer-%E8%AF%B4%E6%98%8E%E5%8F%8A%E9%80%89%E6%8B%A9%E6%97%B6%E6%9C%BA"><span class="nav-number">2.0.1.</span> <span class="nav-text">三种 writer 说明及选择时机</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Serializer%E6%94%AF%E6%8C%81relocation"><span class="nav-number">2.0.2.</span> <span class="nav-text">Serializer支持relocation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BypassMergeSortShuffleWriter"><span class="nav-number">2.1.</span> <span class="nav-text">BypassMergeSortShuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#UnsafeSuffleWriter"><span class="nav-number">2.2.</span> <span class="nav-text">UnsafeSuffleWriter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SortShuffleWriter"><span class="nav-number">2.3.</span> <span class="nav-text">SortShuffleWriter</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#writer-%E6%96%B9%E6%B3%95"><span class="nav-number">2.3.1.</span> <span class="nav-text">writer 方法</span></a></li></ol></li></ol></li></ol></div>
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="sevncz"
      src="/images/IMG_1169.JPG">
  <p class="site-author-name" itemprop="name">sevncz</p>
  <div class="site-description" itemprop="description">努力赚钱</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">20</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Sevncz" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Sevncz" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/IMG_1169.JPG">
      <meta itemprop="name" content="sevncz">
      <meta itemprop="description" content="努力赚钱">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sevncz's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Spark ShuffleManager的运行原理
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-22 15:01:22" itemprop="dateCreated datePublished" datetime="2020-12-22T15:01:22+08:00">2020-12-22</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-01-19 12:14:26" itemprop="dateModified" datetime="2021-01-19T12:14:26+08:00">2021-01-19</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h2 id="什么是Shuffle"><a href="#什么是Shuffle" class="headerlink" title="什么是Shuffle"></a>什么是Shuffle</h2><p>有些运算需要将各节点上的同一类数据汇集到某一节点进行计算，把这些分布在不同节点的数据按照一定的规则汇集到一起的过程称为 Shuffle。</p>
<h2 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h2><p>先看下SortShuffleManager的官方说明</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * In sort-based shuffle, incoming records are sorted according to their target partition ids, then</span></span><br><span class="line"><span class="comment"> * written to a single map output file. Reducers fetch contiguous regions of this file in order to</span></span><br><span class="line"><span class="comment"> * read their portion of the map output. In cases where the map output data is too large to fit in</span></span><br><span class="line"><span class="comment"> * memory, sorted subsets of the output can be spilled to disk and those on-disk files are merged</span></span><br><span class="line"><span class="comment"> * to produce the final output file.</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> # 有两种不同的方式输出文件</span></span><br><span class="line"><span class="comment"> * Sort-based shuffle has two different write paths for producing its map output files:</span></span><br><span class="line"><span class="comment"> # 序列化排序，当一下三个条件都成立时使用</span></span><br><span class="line"><span class="comment"> *  - Serialized sorting: used when all three of the following conditions hold:</span></span><br><span class="line"><span class="comment"> # shuffle 的依赖没有指定的聚合或输出排序</span></span><br><span class="line"><span class="comment"> *    1. The shuffle dependency specifies no aggregation or output ordering.</span></span><br><span class="line"><span class="comment"> # shuffle 的序列化支持序列化值的重定位</span></span><br><span class="line"><span class="comment"> *    2. The shuffle serializer supports relocation of serialized values (this is currently supported by KryoSerializer and Spark SQL&#x27;s custom serializers).</span></span><br><span class="line"><span class="comment"> # shuffle 产生的输出分区少于16777216</span></span><br><span class="line"><span class="comment"> *    3. The shuffle produces fewer than 16777216 output partitions.</span></span><br><span class="line"><span class="comment"> *  - Deserialized sorting: used to handle all other cases.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> * Serialized sorting mode</span></span><br><span class="line"><span class="comment"> * -----------------------</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 优化手段</span></span><br><span class="line"><span class="comment"> * In the serialized sorting mode, incoming records are serialized as soon as they are passed to the</span></span><br><span class="line"><span class="comment"> * shuffle writer and are buffered in a serialized form during sorting. This write path implements</span></span><br><span class="line"><span class="comment"> * several optimizations:</span></span><br><span class="line"><span class="comment"> # 排序在二进制数据上而不是Java对象</span></span><br><span class="line"><span class="comment"> *  - Its sort operates on serialized binary data rather than Java objects, which reduces memory</span></span><br><span class="line"><span class="comment"> *    consumption and GC overheads. This optimization requires the record serializer to have certain</span></span><br><span class="line"><span class="comment"> *    properties to allow serialized records to be re-ordered without requiring deserialization.</span></span><br><span class="line"><span class="comment"> *    See SPARK-4550, where this optimization was first proposed and implemented, for more details.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 使用一个专门的缓存效率高的分拣器([[ShuffleExternalSorter]])，对压缩记录指针和分区ID的数组进行分拣。通过在排序数组中每条记录只使用8个字节的空间，这可以将更多的数组放入缓存。</span></span><br><span class="line"><span class="comment"> *  - It uses a specialized cache-efficient sorter ([[ShuffleExternalSorter]]) that sorts</span></span><br><span class="line"><span class="comment"> *    arrays of compressed record pointers and partition ids. By using only 8 bytes of space per</span></span><br><span class="line"><span class="comment"> *    record in the sorting array, this fits more of the array into cache.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> # 溢出合并程序对属于同一分区的序列化记录块进行操作，在合并过程中不需要反序列化记录。</span></span><br><span class="line"><span class="comment"> *  - The spill merging procedure operates on blocks of serialized records that belong to the same</span></span><br><span class="line"><span class="comment"> *    partition and does not need to deserialize records during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> #  当spill压缩编解码器支持压缩数据的连接时，spill合并只是将序列化和压缩的spill分区连接起来，产生最终的输出分区。 这样就可以使用高效的数据复制方法，如NIO的 &quot;transferTo&quot;，并避免了在合并过程中分配解压或复制缓冲区的需要。</span></span><br><span class="line"><span class="comment"> *  - When the spill compression codec supports concatenation of compressed data, the spill merge</span></span><br><span class="line"><span class="comment"> *    simply concatenates the serialized and compressed spill partitions to produce the final output</span></span><br><span class="line"><span class="comment"> *    partition.  This allows efficient data copying methods, like NIO&#x27;s `transferTo`, to be used</span></span><br><span class="line"><span class="comment"> *    and avoids the need to allocate decompression or copying buffers during the merge.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * For more details on these optimizations, see SPARK-7081.</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure>

<p>主要方法</p>
<ul>
<li>registerShuffle</li>
<li>getReader</li>
<li>getWriter</li>
<li>unregisterShuffle</li>
</ul>
<h4 id="三种-writer-说明及选择时机"><a href="#三种-writer-说明及选择时机" class="headerlink" title="三种 writer 说明及选择时机"></a>三种 writer 说明及选择时机</h4><table>
<thead>
<tr>
<th>类型</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>和Hash Shuffle实现基本相同，区别在于map task输出会汇总为一个文件</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>tungsten-sort，ShuffleExternalSorter使用Java Unsafe直接操作内存，避免Java对象多余的开销和GC 延迟，效率高</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>Sort Shuffle，和Hash Shuffle的主要不同在于，map端支持Partition级别的sort，map task输出会汇总为一个文件</td>
</tr>
</tbody></table>
<p>Spark根据运行时信息选择三种ShuffleWriter实现中的一种，对应的源码为<strong>SortShuffleManager</strong>中的<strong>registerShuffle</strong>方法，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Obtains a [[ShuffleHandle]] to pass to tasks.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      shuffleId: <span class="type">Int</span>,</span><br><span class="line">      numMaps: <span class="type">Int</span>,</span><br><span class="line">      dependency: <span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]): <span class="type">ShuffleHandle</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">SortShuffleWriter</span>.shouldBypassMergeSort(conf, dependency)) &#123;</span><br><span class="line">      <span class="comment">// If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#x27;t</span></span><br><span class="line">      <span class="comment">// need map-side aggregation, then write numPartitions files directly and just concatenate</span></span><br><span class="line">      <span class="comment">// them at the end. This avoids doing serialization and deserialization twice to merge</span></span><br><span class="line">      <span class="comment">// together the spilled files, which would happen with the normal code path. The downside is</span></span><br><span class="line">      <span class="comment">// having multiple files open at a time and thus more memory allocated to buffers.</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BypassMergeSortShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">SortShuffleManager</span>.canUseSerializedShuffle(dependency)) &#123;</span><br><span class="line">      <span class="comment">// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializedShuffleHandle</span>[<span class="type">K</span>, <span class="type">V</span>](</span><br><span class="line">        shuffleId, numMaps, dependency.asInstanceOf[<span class="type">ShuffleDependency</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>]])</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Otherwise, buffer map outputs in a deserialized form:</span></span><br><span class="line">      <span class="keyword">new</span> <span class="type">BaseShuffleHandle</span>(shuffleId, numMaps, dependency)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>选择逻辑如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>map-side aggregation</th>
<th>Partition数(RDD)</th>
<th>Serializer支持relocation</th>
</tr>
</thead>
<tbody><tr>
<td>BypassMergeSortShuffleWriter</td>
<td>否</td>
<td>小于200(默认)</td>
<td>-</td>
</tr>
<tr>
<td>UnsafeShuffleWriter</td>
<td>否</td>
<td>小于16777216</td>
<td>是</td>
</tr>
<tr>
<td>SortShuffleWriter</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody></table>
<ul>
<li><p>BypassMergeSortShuffleWriter</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shouldBypassMergeSort</span></span>(conf: <span class="type">SparkConf</span>, dep: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="comment">// We cannot bypass sorting if we need to do map-side aggregation.</span></span><br><span class="line">    <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bypassMergeThreshold: <span class="type">Int</span> = conf.getInt(<span class="string">&quot;spark.shuffle.sort.bypassMergeThreshold&quot;</span>, <span class="number">200</span>)</span><br><span class="line">      dep.partitioner.numPartitions &lt;= bypassMergeThreshold</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<ol>
<li>判断是否开启combine，只能在未开启combine时使用；</li>
<li>分区数量小于 spark.shuffle.sort.bypassMergeThreshold 设置的阈值；</li>
</ol>
</li>
<li><p>UnsafeSuffleWriter</p>
<p>canUseSerializedShuffle 判断为 ture</p>
<ol>
<li> serializer 支持 relocation；</li>
<li>没有定义本地combine</li>
<li>分区数量小于16777216</li>
</ol>
</li>
<li><p>SortShuffleWriter</p>
<p>canUseSerializedShuffle 判断为false</p>
<ol>
<li> serializer 不支持 relocation；</li>
<li>定义了本地combine</li>
<li>分区数量大于 16777216</li>
</ol>
</li>
</ul>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">canUseSerializedShuffle</span></span>(dependency: <span class="type">ShuffleDependency</span>[_, _, _]): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> shufId = dependency.shuffleId</span><br><span class="line">  <span class="keyword">val</span> numPartitions = dependency.partitioner.numPartitions</span><br><span class="line">  <span class="comment">// 判断 serializer 是否支持 relocation</span></span><br><span class="line">  <span class="keyword">if</span> (!dependency.serializer.supportsRelocationOfSerializedObjects) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because the serializer, &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;dependency.serializer.getClass.getName&#125;</span>, does not support object relocation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 是否是本地combine</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (dependency.mapSideCombine) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because we need to do &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;map-side aggregation&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  <span class="comment">// 分区数量是否 &gt; 16777215 + 1</span></span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (numPartitions &gt; <span class="type">MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span>) &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can&#x27;t use serialized shuffle for shuffle <span class="subst">$shufId</span> because it has more than &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE</span> partitions&quot;</span>)</span><br><span class="line">    <span class="literal">false</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    log.debug(<span class="string">s&quot;Can use serialized shuffle for shuffle <span class="subst">$shufId</span>&quot;</span>)</span><br><span class="line">    <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Serializer支持relocation"><a href="#Serializer支持relocation" class="headerlink" title="Serializer支持relocation"></a>Serializer支持relocation</h4><p>Serializer可以对已经序列化的对象进行排序，这种排序起到的效果和先对数据排序再序列化一致。Serializer的这个属性会在UnsafeShuffleWriter进行排序时用到。</p>
<p>支持relocation的Serializer是KryoSerializer，Spark默认使用<strong>JavaSerializer</strong>，通过参数spark.serializer设置。</p>
<h3 id="BypassMergeSortShuffleWriter"><a href="#BypassMergeSortShuffleWriter" class="headerlink" title="BypassMergeSortShuffleWriter"></a>BypassMergeSortShuffleWriter</h3><p>map端结果按照bucket顺序依次写入<strong>dataFile</strong>文件中，这么处理后，Shuffle生成的文件数显著减少了，同时还会生成indexFile文件，记录各个bucket在dataFile中的位置，用于后续reducer随机读取文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">public void write(<span class="type">Iterator</span>&lt;<span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt;&gt; records) <span class="keyword">throws</span> <span class="type">IOException</span> &#123;</span><br><span class="line">    assert (partitionWriters == <span class="literal">null</span>);</span><br><span class="line">    <span class="keyword">if</span> (!records.hasNext()) &#123;</span><br><span class="line">      partitionLengths = <span class="keyword">new</span> long[numPartitions];</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, <span class="literal">null</span>);</span><br><span class="line">      mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">      <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">final</span> <span class="type">SerializerInstance</span> serInstance = serializer.newInstance();</span><br><span class="line">    <span class="keyword">final</span> long openStartTime = <span class="type">System</span>.nanoTime();</span><br><span class="line">    partitionWriters = <span class="keyword">new</span> <span class="type">DiskBlockObjectWriter</span>[numPartitions];</span><br><span class="line">    partitionWriterSegments = <span class="keyword">new</span> <span class="type">FileSegment</span>[numPartitions];</span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Tuple2</span>&lt;<span class="type">TempShuffleBlockId</span>, <span class="type">File</span>&gt; tempShuffleBlockIdPlusFile =</span><br><span class="line">        blockManager.diskBlockManager().createTempShuffleBlock();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">File</span> file = tempShuffleBlockIdPlusFile._2();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">BlockId</span> blockId = tempShuffleBlockIdPlusFile._1();</span><br><span class="line">      partitionWriters[i] =</span><br><span class="line">        blockManager.getDiskWriter(blockId, file, serInstance, fileBufferSize, writeMetrics);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Creating the file to write to and creating a disk writer both involve interacting with</span></span><br><span class="line">    <span class="comment">// the disk, and can take a long time in aggregate when we open many files, so should be</span></span><br><span class="line">    <span class="comment">// included in the shuffle write time.</span></span><br><span class="line">    writeMetrics.incWriteTime(<span class="type">System</span>.nanoTime() - openStartTime);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (records.hasNext()) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">Product2</span>&lt;<span class="type">K</span>, <span class="type">V</span>&gt; record = records.next();</span><br><span class="line">      <span class="keyword">final</span> <span class="type">K</span> key = record._1();</span><br><span class="line">      partitionWriters[partitioner.getPartition(key)].write(key, record._2());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; numPartitions; i++) &#123;</span><br><span class="line">      <span class="keyword">final</span> <span class="type">DiskBlockObjectWriter</span> writer = partitionWriters[i];</span><br><span class="line">      partitionWriterSegments[i] = writer.commitAndGet();</span><br><span class="line">      writer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="type">File</span> output = shuffleBlockResolver.getDataFile(shuffleId, mapId);</span><br><span class="line">    <span class="type">File</span> tmp = <span class="type">Utils</span>.tempFileWith(output);</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      partitionLengths = writePartitionedFile(tmp);</span><br><span class="line">      shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, tmp);</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">        logger.error(<span class="string">&quot;Error while deleting temp file &#123;&#125;&quot;</span>, tmp.getAbsolutePath());</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>$.<span class="type">MODULE</span>$.apply(blockManager.shuffleServerId(), partitionLengths);</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>



<h3 id="UnsafeSuffleWriter"><a href="#UnsafeSuffleWriter" class="headerlink" title="UnsafeSuffleWriter"></a>UnsafeSuffleWriter</h3><p>UnsafeShuffleWriter内部使用了和<a target="_blank" rel="noopener" href="http://blog.csdn.net/u011564172/article/details/71170205">BytesToBytesMap</a>基本相同的数据结构处理map端的输出，不过将其细化为<strong>ShuffleExternalSorter</strong>和<strong>ShuffleInMemorySorter</strong>两部分，功能如下</p>
<ul>
<li>ShuffleExternalSorter：使用MemoryBlock存储数据，每条记录包括长度信息和K-V Pair</li>
<li>ShuffleInMemorySorter：使用long数组存储每条记录对应的位置信息(page number + offset)，以及其对应的PartitionId，共8 bytes</li>
</ul>
<h3 id="SortShuffleWriter"><a href="#SortShuffleWriter" class="headerlink" title="SortShuffleWriter"></a>SortShuffleWriter</h3><p>SortShuffleWriter它主要是判断在Map端是否需要本地进行combine操作。如果需要聚合，则使用PartitionedAppendOnlyMap；如果不进行combine操作，则使用PartitionedPairBuffer添加数据存放于内存中。然后无论哪一种情况都需要判断内存是否足够，如果内存不够而且又申请不到内存，则需要进行本地磁盘溢写操作，把相关的数据写入溢写到临时文件。最后把内存里的数据和磁盘溢写的临时文件的数据进行合并，如果需要则进行一次归并排序，如果没有发生溢写则是不需要归并排序，因为都在内存里。最后生成合并后的data文件和index文件。</p>
<h4 id="writer-方法"><a href="#writer-方法" class="headerlink" title="writer 方法"></a>writer 方法</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Write a bunch of records to this task&#x27;s output */</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">write</span></span>(records: <span class="type">Iterator</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 1. 判断是否有map端combine</span></span><br><span class="line">  sorter = <span class="keyword">if</span> (dep.mapSideCombine) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">      context, dep.aggregator, <span class="type">Some</span>(dep.partitioner), dep.keyOrdering, dep.serializer)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// In this case we pass neither an aggregator nor an ordering to the sorter, because we don&#x27;t</span></span><br><span class="line">    <span class="comment">// care whether the keys get sorted in each partition; that will be done on the reduce side</span></span><br><span class="line">    <span class="comment">// if the operation being run is sortByKey.</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ExternalSorter</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](</span><br><span class="line">      context, aggregator = <span class="type">None</span>, <span class="type">Some</span>(dep.partitioner), ordering = <span class="type">None</span>, dep.serializer)</span><br><span class="line">  &#125;</span><br><span class="line">  sorter.insertAll(records)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Don&#x27;t bother including the time to open the merged output file in the shuffle write time,</span></span><br><span class="line">  <span class="comment">// because it just opens a single file, so is typically too fast to measure accurately</span></span><br><span class="line">  <span class="comment">// (see SPARK-3570).</span></span><br><span class="line">  <span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line">  <span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> blockId = <span class="type">ShuffleBlockId</span>(dep.shuffleId, mapId, <span class="type">IndexShuffleBlockResolver</span>.<span class="type">NOOP_REDUCE_ID</span>)</span><br><span class="line">    <span class="keyword">val</span> partitionLengths = sorter.writePartitionedFile(blockId, tmp)</span><br><span class="line">    shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)</span><br><span class="line">    mapStatus = <span class="type">MapStatus</span>(blockManager.shuffleServerId, partitionLengths)</span><br><span class="line">  &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">if</span> (tmp.exists() &amp;&amp; !tmp.delete()) &#123;</span><br><span class="line">      logError(<span class="string">s&quot;Error while deleting temp file <span class="subst">$&#123;tmp.getAbsolutePath&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol>
<li><p>创建外部排序器ExternalSorter, 只是根据是否需要本地combine与否从而决定是否传入aggregator和keyOrdering参数；</p>
</li>
<li><p>调用ExternalSorter实例的insertAll方法，插入record；如果ExternalSorter实例中用以保存record的in-memory collection的大小达到阈值，会将record按顺序溢写到磁盘文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Spill the current in-memory collection to disk if needed.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param usingMap whether we&#x27;re using a map or buffer as our current in-memory collection</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">maybeSpillCollection</span></span>(usingMap: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> estimatedSize = <span class="number">0</span>L</span><br><span class="line">  <span class="keyword">if</span> (usingMap) &#123;</span><br><span class="line">    estimatedSize = map.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(map, estimatedSize)) &#123;</span><br><span class="line">      map = <span class="keyword">new</span> <span class="type">PartitionedAppendOnlyMap</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    estimatedSize = buffer.estimateSize()</span><br><span class="line">    <span class="keyword">if</span> (maybeSpill(buffer, estimatedSize)) &#123;</span><br><span class="line">      buffer = <span class="keyword">new</span> <span class="type">PartitionedPairBuffer</span>[<span class="type">K</span>, <span class="type">C</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   </span><br><span class="line">  <span class="keyword">if</span> (estimatedSize &gt; _peakMemoryUsedBytes) &#123;</span><br><span class="line">    _peakMemoryUsedBytes = estimatedSize</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



</li>
</ol>
<ol start="3">
<li><p>构造最终的输出文件实例,其中文件名为(reduceId为0)： “shuffle_” + shuffleId + “<em>“ + mapId + “</em>“ + reduceId；</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> output = shuffleBlockResolver.getDataFile(dep.shuffleId, mapId)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">ShuffleDataBlockId</span>(<span class="params">shuffleId: <span class="type">Int</span>, mapId: <span class="type">Int</span>, reduceId: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">BlockId</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">name</span></span>: <span class="type">String</span> = <span class="string">&quot;shuffle_&quot;</span> + shuffleId + <span class="string">&quot;_&quot;</span> + mapId + <span class="string">&quot;_&quot;</span> + reduceId + <span class="string">&quot;.data&quot;</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在输出文件名后加上uuid用于标识文件正在写入，结束后重命名</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> tmp = <span class="type">Utils</span>.tempFileWith(output)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tempFileWith</span></span>(path: <span class="type">File</span>): <span class="type">File</span> = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">File</span>(path.getAbsolutePath + <span class="string">&quot;.&quot;</span> + <span class="type">UUID</span>.randomUUID())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>调用 ExternalSorter 实例的 <code>writePartitionedFile</code> 方法，将插入到该 sorter 的 record 进行排序并写入输出文件；插入到 sorter 的 record 可以是在 in-memory collection 或者在溢写文件。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Write all the data added into this ExternalSorter into a file in the disk store. This is</span></span><br><span class="line"><span class="comment">   * called by the SortShuffleWriter.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param blockId block ID to write to. The index file will be blockId.name + &quot;.index&quot;.</span></span><br><span class="line"><span class="comment">   * @return array of lengths, in bytes, of each partition of the file (used by map output tracker)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writePartitionedFile</span></span>(</span><br><span class="line">      blockId: <span class="type">BlockId</span>,</span><br><span class="line">      outputFile: <span class="type">File</span>): <span class="type">Array</span>[<span class="type">Long</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Track location of each range in the output file</span></span><br><span class="line">    <span class="keyword">val</span> lengths = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Long</span>](numPartitions)</span><br><span class="line">    <span class="keyword">val</span> writer = blockManager.getDiskWriter(blockId, outputFile, serInstance, fileBufferSize,</span><br><span class="line">      context.taskMetrics().shuffleWriteMetrics)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (spills.isEmpty) &#123;</span><br><span class="line">      <span class="comment">// Case where we only have in-memory data</span></span><br><span class="line">      <span class="keyword">val</span> collection = <span class="keyword">if</span> (aggregator.isDefined) map <span class="keyword">else</span> buffer</span><br><span class="line">      <span class="keyword">val</span> it = collection.destructiveSortedWritablePartitionedIterator(comparator)</span><br><span class="line">      <span class="keyword">while</span> (it.hasNext) &#123;</span><br><span class="line">        <span class="keyword">val</span> partitionId = it.nextPartition()</span><br><span class="line">        <span class="keyword">while</span> (it.hasNext &amp;&amp; it.nextPartition() == partitionId) &#123;</span><br><span class="line">          it.writeNext(writer)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">        lengths(partitionId) = segment.length</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We must perform merge-sort; get an iterator by partition and write everything directly.</span></span><br><span class="line">      <span class="keyword">for</span> ((id, elements) &lt;- <span class="keyword">this</span>.partitionedIterator) &#123;</span><br><span class="line">        <span class="keyword">if</span> (elements.hasNext) &#123;</span><br><span class="line">          <span class="keyword">for</span> (elem &lt;- elements) &#123;</span><br><span class="line">            writer.write(elem._1, elem._2)</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> segment = writer.commitAndGet()</span><br><span class="line">          lengths(id) = segment.length</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    writer.close()</span><br><span class="line">    context.taskMetrics().incMemoryBytesSpilled(memoryBytesSpilled)</span><br><span class="line">    context.taskMetrics().incDiskBytesSpilled(diskBytesSpilled)</span><br><span class="line">    context.taskMetrics().incPeakExecutionMemory(peakMemoryUsedBytes)</span><br><span class="line"></span><br><span class="line">    lengths</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>将每个 partition 的 offset 写入 index 文件方便 reduce 端 fetch 数据；</p>
</li>
<li><p>把部分信息封装到MapStatus返回；</p>
</li>
</ol>

    </div>

    
    
    
      
  <div class="popular-posts-header">相关文章</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/12/14/Spark Streaming运行的基本原理/" rel="bookmark">Spark Streaming运行的基本原理</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/12/08/Spark SQL源码分析/" rel="bookmark">Spark SQL 源码分析</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/12/07/Spark SQL运行的基本原理/" rel="bookmark">Spark SQL运行的基本原理</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/12/05/Spark运行的基本原理/" rel="bookmark">Spark 运行的基本原理</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/12/04/Hive运行的基本原理/" rel="bookmark">Hive运行的基本原理</a></div>
    </li>
  </ul>


    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"><i class="fa fa-tag"></i> 大数据</a>
              <a href="/tags/spark/" rel="tag"><i class="fa fa-tag"></i> spark</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/12/22/%E6%97%81%E8%B7%AF%E7%BC%93%E5%AD%98%EF%BC%9ARedis%E7%BC%93%E5%AD%98%E5%BA%94%E7%94%A8/" rel="prev" title="旁路缓存：Redis缓存应用">
                  <i class="fa fa-chevron-left"></i> 旁路缓存：Redis缓存应用
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/12/22/Redis-%E4%B8%BB%E4%BB%8E%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E5%8E%9F%E7%90%86/" rel="next" title="Redis 主从数据同步原理">
                  Redis 主从数据同步原理 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>







<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sevncz@xyz</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/pisces/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/bookmark.js"></script>

  







  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <script src="//cdn.jsdelivr.net/npm/quicklink@2.0.0/dist/quicklink.umd.js"></script>
  <script>
      window.addEventListener('load', () => {
      quicklink.listen({
        timeout : 3000,
        priority: true,
        ignores : [uri => uri.includes('#'),uri => uri === 'http://sevncz.xyz/2020/12/22/Spark-ShuffleManager%E7%9A%84%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86/',]
      });
      });
  </script>

</body>
</html>
